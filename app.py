import streamlit as st
import pandas as pd
import re
from collections import Counter

# 1. í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ë„¤ì´ë²„ SEO NLU ë§ˆìŠ¤í„°", layout="wide")
st.title("ğŸš€ ë„¤ì´ë²„ ì‡¼í•‘ SEO í†µí•© ìµœì í™” ë§¤ë‹ˆì €")
st.markdown("---")

# 2. ì „ë¬¸ SEO ë¶„ì„ ë¡œì§ í´ë˜ìŠ¤
class SEOManager:
    def __init__(self, df, user_exclude_list):
        self.df = df
        # ê¸°ë³¸ ë¸Œëœë“œ ì œì™¸ ë¦¬ìŠ¤íŠ¸ + ì‚¬ìš©ì ì…ë ¥ ì œì™¸ í‚¤ì›Œë“œ í†µí•©
        self.exclude_brands = [
            'ë§¤ì¼', 'ì„œìš¸ìš°ìœ ', 'ì„œìš¸', 'ì—°ì„¸', 'ë‚¨ì–‘', 'ê±´êµ­', 'íŒŒìŠ¤í‡´ë¥´', 'ì¼ë™', 'í›„ë””ìŠ¤', 
            'ì†Œì™€ë‚˜ë¬´', 'ë¹™ê·¸ë ˆ', 'ì…€ë¡œëª¬', 'ë¹…ì›ë”', 'ë¯¸ê´‘ìŠ¤í† ì–´', 'ë°ì–´ë¦¬ë§ˆì¼“', 'ë„ë‚¨ìƒíšŒ', 
            'í¬ì°½ìœ ì—…', 'ë‹´í„°', 'ì—°ì„¸ìœ ì—…', 'ë§¤ì¼ìœ ì—…'
        ] + user_exclude_list

    def split_base_terms(self, text):
        """ë³µí•© ëª…ì‚¬ ë¶„ë¦¬ ë° ìˆ˜ì¹˜ê°’/ë¸Œëœë“œ/ì œì™¸ì–´ ì œê±°"""
        if pd.isna(text) or text == '-': return []
        text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', ' ', str(text))
        raw_words = text.split()
        
        terms = []
        sub_splits = ['ìíŒê¸°', 'ìš°ìœ ', 'ë¶„ìœ ', 'ê°€ë£¨', 'ë¶„ë§', 'ì „ì§€', 'íƒˆì§€', 'ìŠ¤í‹±', 'ì—…ì†Œìš©', 'ëŒ€ìš©ëŸ‰']
        
        for word in raw_words:
            # ì œì™¸ ë¦¬ìŠ¤íŠ¸ í¬í•¨ ì—¬ë¶€ ë° ìˆ«ì í¬í•¨ ì—¬ë¶€ ì²´í¬
            if word in self.exclude_brands or any(char.isdigit() for char in word):
                continue
            
            found_sub = False
            for sub in sub_splits:
                if sub in word and word != sub:
                    terms.append(sub)
                    rem = word.replace(sub, '').strip()
                    if len(rem) > 1 and not any(char.isdigit() for char in rem) and rem not in self.exclude_brands:
                        terms.append(rem)
                    found_sub = True
                    break
            if not found_sub and len(word) > 1:
                terms.append(word)
        return terms

    def reorder_for_readability(self, word_count_pairs):
        """ê°€ë…ì„± ê·¸ë£¹ë³„ ì¬ë°°ì¹˜"""
        identity = ['ì „ì§€', 'ë¶„ìœ ', 'ìš°ìœ ', 'íƒˆì§€', 'ì „ì§€ë°€']
        form = ['ë¶„ë§', 'ê°€ë£¨', 'ìŠ¤í‹±', 'ì•¡ìƒ']
        usage = ['ìíŒê¸°', 'ì—…ì†Œìš©', 'ëŒ€ìš©ëŸ‰', 'ì‹ìì¬', 'ì œê³¼', 'ì œë¹µ', 'ë² ì´í‚¹']
        desc = ['ì§„í•œ', 'ê³ ì†Œí•œ', 'ë§›ìˆëŠ”', 'ì¶”ì–µ', 'ì¶”ì²œ', 'ì†í¸í•œ']

        def get_priority(pair):
            word = pair[0]
            if any(core in word for core in identity): return 1
            if any(core in word for core in form): return 2
            if any(core in word for core in usage): return 3
            if any(core in word for core in desc): return 4
            return 5

        return sorted(word_count_pairs, key=lambda x: get_priority(x))

    def run_analysis(self, manual_input, add_input, total_target_count):
        # ìˆ˜ë™ ì…ë ¥(ìœ ì…) ë° ì¶”ê°€ í¬ë§ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸í™”
        manual_keywords = [w.strip() for w in manual_input.split() if len(w.strip()) > 0]
        add_keywords = [w.strip() for w in add_input.split() if len(w.strip()) > 0]
        
        # ê³ ì • ë°°ì¹˜ í‚¤ì›Œë“œ (ìœ ì… + ì¶”ê°€)
        fixed_keywords = manual_keywords + add_keywords
        
        # [1] ìƒí’ˆëª… ë¶„ì„
        name_terms = []
        for name in self.df['ìƒí’ˆëª…']:
            name_terms.extend(self.split_base_terms(name))
        
        name_freq = Counter(name_terms).most_common(50)
        auto_candidates = []
        for w, c in name_freq:
            # ê³ ì • í‚¤ì›Œë“œì™€ ì¤‘ë³µë˜ê±°ë‚˜ ì˜ë¯¸ê°€ ê²¹ì¹˜ëŠ” ë‹¨ì–´ ì œì™¸
            if not any(fixed_w in w or w in fixed_w for fixed_w in fixed_keywords):
                auto_candidates.append((w, c))
        
        # ì´ í‚¤ì›Œë“œ ìˆ˜ì—ì„œ ê³ ì • í‚¤ì›Œë“œ ìˆ˜ë¥¼ ëº€ ë‚˜ë¨¸ì§€ë§Œ AIê°€ ì±„ì›€
        remain_count = max(0, total_target_count - len(fixed_keywords))
        selected_auto_pairs = auto_candidates[:remain_count]
        readable_auto_pairs = self.reorder_for_readability(selected_auto_pairs)
        
        # [2] ì†ì„± ë¶„ì„
        spec_list = []
        for spec in self.df['ìŠ¤í™'].dropna():
            if spec != '-':
                parts = [p.strip() for p in str(spec).split('|')]
                spec_list.extend([p for p in parts if len(p) > 1 and p not in self.exclude_brands])
        spec_counts = Counter(spec_list).most_common(8)

        # [3] íƒœê·¸ ë¶„ì„
        tag_raw_list = []
        for tags in self.df['ê²€ìƒ‰ì¸ì‹íƒœê·¸'].dropna():
            if tags != '-':
                parts = [t.strip() for t in str(tags).split(',')]
                tag_raw_list.extend([t for t in parts if not any(b in t for b in self.exclude_brands)])
        
        tag_freq = Counter(tag_raw_list).most_common(150)
        current_title_words = fixed_keywords + [p[0] for p in readable_auto_pairs]
        
        valid_candidates = []
        for t, c in tag_freq:
            if not any(char.isdigit() for char in t) and not any(word in t for word in current_title_words):
                valid_candidates.append((t, c))

        final_tags = []
        used_roots = set()
        clusters = {
            'ì œê³¼': ['ì œê³¼', 'ì œë¹µ', 'ë² ì´í‚¹', 'ìš©í’ˆ', 'ì¬ë£Œ', 'í™ˆë² ì´í‚¹'],
            'ë§›': ['ë§›', 'ë‹¬ë‹¬', 'ë¶€ë“œëŸ¬ìš´', 'ê³ ì†Œ', 'ì§„í•œ'],
            'ì˜ì–‘': ['ì˜ì–‘', 'ë‹¨ë°±ì§ˆ', 'ê±´ê°•', 'ëª¸ì—ì¢‹ì€'],
            'ì°¨': ['ì°¨', 'ìŒë£Œ', 'ì»¤í”¼', 'í‹°'],
            'ê°„ì‹': ['ê°„ì‹', 'ì£¼ì „ë¶€ë¦¬'],
            'ìš©ë„': ['ìíŒê¸°', 'ì‹ìì¬', 'ìš”ë¦¬']
        }

        for t, c in valid_candidates:
            matched_root = None
            for root, keywords in clusters.items():
                if any(k in t for k in keywords):
                    matched_root = root
                    break
            if matched_root and matched_root not in used_roots:
                final_tags.append((t, c))
                used_roots.add(matched_root)

        for t, c in valid_candidates:
            if len(final_tags) >= 10: break
            if any(t == existing[0] for existing in final_tags): continue
            is_redundant = False
            for existing_t, _ in final_tags:
                if t in existing_t or existing_t in t:
                    is_redundant = True
                    break
            if not is_redundant:
                final_tags.append((t, c))
        
        final_tags = sorted(final_tags, key=lambda x: x[1], reverse=True)[:10]
        
        return fixed_keywords, readable_auto_pairs, spec_counts, final_tags

# 3. ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ (GUI)
st.sidebar.header("ğŸ“ Step 1. ë°ì´í„° ì—…ë¡œë“œ")
uploaded_file = st.sidebar.file_uploader("ë¶„ì„ìš© CSV íŒŒì¼ ì—…ë¡œë“œ", type=["csv"])

st.sidebar.header("ğŸ¯ Step 2. ìƒí’ˆëª… í‚¤ì›Œë“œ ì»¤ìŠ¤í…€")
manual_input = st.sidebar.text_input("ìœ ì… í‚¤ì›Œë“œ (êµ¬ë§¤ ìœ ë„)", placeholder="ì˜ˆ: ë§›ìˆëŠ” ì†í¸í•œ")
add_input = st.sidebar.text_input("ì¶”ê°€í•  í‚¤ì›Œë“œ (ê³ ì • ë°°ì¹˜)", placeholder="ì˜ˆ: êµ­ë‚´ì‚° ë‹¹ì¼ë°œì†¡")
exclude_input = st.sidebar.text_input("ì œì™¸í•  í‚¤ì›Œë“œ (ë¶„ì„ ì œì™¸)", placeholder="ì˜ˆ: ë¬´ì„¤íƒ• ë¹„ê±´")

total_kw_count = st.sidebar.number_input("ìƒí’ˆëª… ì´ í‚¤ì›Œë“œ ìˆ˜", min_value=5, max_value=25, value=12)

# ì‚¬ìš©ì ì œì™¸ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸í™”
user_exclude_list = [w.strip() for w in exclude_input.split() if len(w.strip()) > 0]

if uploaded_file:
    df = None
    try:
        df = pd.read_csv(uploaded_file, encoding='cp949')
    except:
        uploaded_file.seek(0)
        df = pd.read_csv(uploaded_file, encoding='utf-8-sig')

    if df is not None:
        manager = SEOManager(df, user_exclude_list)
        # ë¶„ì„ í•¨ìˆ˜ì— ì¶”ê°€ í‚¤ì›Œë“œ(add_input)ë„ ì „ë‹¬
        fixed_keys, auto_keys_pairs, specs, tags = manager.run_analysis(manual_input, add_input, total_kw_count)

        st.success(f"âœ¨ ì„¤ì •í•˜ì‹  ì¡°ê±´ì— ë”°ë¥¸ ìµœì í™” ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

        # ì„¹ì…˜ 1: ìƒí’ˆëª…
        st.header("ğŸ·ï¸ 1. ì „ëµì  ìƒí’ˆëª… ì¡°í•©")
        col1, col2 = st.columns([2, 1])
        with col1:
            st.subheader("âœ… ì™„ì„±ëœ ìƒí’ˆëª…")
            full_title = " ".join(fixed_keys + [p[0] for p in auto_keys_pairs])
            st.code(full_title, language=None)
            st.caption(f"ê³ ì • í‚¤ì›Œë“œ(ìœ ì…+ì¶”ê°€) {len(fixed_keys)}ê°œ + AI ìë™ í‚¤ì›Œë“œ {len(auto_keys_pairs)}ê°œ")
        with col2:
            st.subheader("ğŸ“Š ìë™ í‚¤ì›Œë“œ ë¹ˆë„ (ì œì™¸ì–´ ë°˜ì˜)")
            auto_df = pd.DataFrame(auto_keys_pairs, columns=['ë‹¨ì–´', 'ë¹ˆë„(íšŒ)'])
            auto_df.index = auto_df.index + 1
            st.table(auto_df)

        st.markdown("---")

        # ì„¹ì…˜ 2: ì†ì„±
        st.header("âš™ï¸ 2. í•„í„° ë…¸ì¶œìš© ì†ì„±ê°’")
        col3, col4 = st.columns([2, 1])
        with col3:
            for s, c in specs: st.button(f"{s}", key=f"attr_{s}", use_container_width=True)
        with col4:
            spec_df = pd.DataFrame(specs, columns=['ì†ì„±ê°’', 'ë¹ˆë„'])
            spec_df.index = spec_df.index + 1
            st.table(spec_df)

        st.markdown("---")

        # ì„¹ì…˜ 3: íƒœê·¸
        st.header("ğŸ” 3. í™•ì¥ ê²€ìƒ‰ íƒœê·¸")
        col5, col6 = st.columns([2, 1])
        with col5:
            st.subheader("âœ… ìµœì í™” íƒœê·¸ 10ì„ ")
            tag_display = ", ".join([f"#{t[0]}" for t in tags])
            st.warning(tag_display)
            st.info(f"**ì œì™¸ ë°˜ì˜:** '{exclude_input}'ì— ì…ë ¥ëœ ë‹¨ì–´ë“¤ì€ íƒœê·¸ì—ì„œë„ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
        with col6:
            st.subheader("ğŸ“Š íƒœê·¸ ì¸ì‹ ë°ì´í„°")
            tag_df = pd.DataFrame(tags, columns=['íƒœê·¸ëª…', 'ì¸ì‹ íšŸìˆ˜'])
            tag_df.index = tag_df.index + 1
            st.table(tag_df)
else:
    st.info("íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì „ëµ í‚¤ì›Œë“œë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
