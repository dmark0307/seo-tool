import streamlit as st
import pandas as pd
import re
from collections import Counter

# 1. í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ë„¤ì´ë²„ SEO NLU í†µí•© ë¶„ì„ê¸°", layout="wide")
st.title("ğŸš€ ë„¤ì´ë²„ ì‡¼í•‘ SEO í†µí•© ìµœì í™” ë§¤ë‹ˆì € (NLU Term ê¸°ë°˜)")
st.markdown("---")

# 2. ì „ë¬¸ SEO ë¶„ì„ ë¡œì§ í´ë˜ìŠ¤
class SEOManager:
    def __init__(self, df):
        self.df = df
        self.exclude_brands = [
            'ë§¤ì¼', 'ì„œìš¸ìš°ìœ ', 'ì„œìš¸', 'ì—°ì„¸', 'ë‚¨ì–‘', 'ê±´êµ­', 'íŒŒìŠ¤í‡´ë¥´', 'ì¼ë™', 'í›„ë””ìŠ¤', 
            'ì†Œì™€ë‚˜ë¬´', 'ë¹™ê·¸ë ˆ', 'ì…€ë¡œëª¬', 'ë¹…ì›ë”', 'ë¯¸ê´‘ìŠ¤í† ì–´', 'ë°ì–´ë¦¬ë§ˆì¼“', 'ë„ë‚¨ìƒíšŒ', 
            'í¬ì°½ìœ ì—…', 'ë‹´í„°', 'ì—°ì„¸ìœ ì—…', 'ë§¤ì¼ìœ ì—…'
        ]
        # NLU ë¶„ì„ ì‹œ í•µì‹¬ ì˜ë¯¸ ë‹¨ìœ„(Term) ì‚¬ì „
        self.core_terms = ['ì „ì§€', 'íƒˆì§€', 'ë¶„ìœ ', 'ìš°ìœ ', 'ê°€ë£¨', 'ë¶„ë§', 'ì œê³¼', 'ì œë¹µ', 'ë² ì´í‚¹', 'ìíŒê¸°', 'ì—…ì†Œìš©', 'ì‹ìì¬']

    def extract_nlu_terms(self, text):
        """í…ìŠ¤íŠ¸ë¥¼ NLU ì˜ë¯¸ ë‹¨ìœ„(Term)ë¡œ ë¶„í•´"""
        if pd.isna(text) or text == '-': return set()
        text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', ' ', str(text))
        words = text.split()
        
        terms = set()
        for word in words:
            # ë¸Œëœë“œ ë° ìˆ˜ì¹˜ê°’ ì œì™¸
            if word in self.exclude_brands or any(char.isdigit() for char in word):
                continue
            # í•µì‹¬ ì‚¬ì „ ê¸°ë°˜ ë¶„í•´ ë° 2ê¸€ì ì´ìƒ ì¶”ì¶œ
            for core in self.core_terms:
                if core in word:
                    terms.add(core)
            if len(word) > 1:
                terms.add(word)
        return terms

    def reorder_for_readability(self, word_count_pairs):
        """ê°€ë…ì„± ê·¸ë£¹ë³„ ì¬ë°°ì¹˜"""
        identity = ['ì „ì§€', 'ë¶„ìœ ', 'ìš°ìœ ', 'íƒˆì§€']
        form = ['ë¶„ë§', 'ê°€ë£¨', 'ìŠ¤í‹±', 'ì•¡ìƒ']
        usage = ['ìíŒê¸°', 'ì—…ì†Œìš©', 'ëŒ€ìš©ëŸ‰', 'ì‹ìì¬', 'ì œê³¼', 'ì œë¹µ', 'ë² ì´í‚¹']
        desc = ['ì§„í•œ', 'ê³ ì†Œí•œ', 'ë§›ìˆëŠ”', 'ì¶”ì–µ', 'ì¶”ì²œ']

        def get_priority(pair):
            word = pair[0]
            if any(core in word for core in identity): return 1
            if any(core in word for core in form): return 2
            if any(core in word for core in usage): return 3
            if any(core in word for core in desc): return 4
            return 5

        return sorted(word_count_pairs, key=lambda x: get_priority(x))

    def run_analysis(self, manual_input):
        manual_keywords = [w.strip() for w in manual_input.split() if len(w.strip()) > 0]
        manual_terms = set()
        for mk in manual_keywords:
            manual_terms.update(self.extract_nlu_terms(mk))
        
        # [1] ìƒí’ˆëª… ë¶„ì„
        all_name_terms = []
        for name in self.df['ìƒí’ˆëª…']:
            found = self.extract_nlu_terms(name)
            all_name_terms.extend([t for t in found if t not in manual_terms])
        
        name_freq = Counter(all_name_terms).most_common(50)
        remain_count = max(0, 12 - len(manual_keywords))
        selected_auto = name_freq[:remain_count]
        readable_auto = self.reorder_for_readability(selected_auto)
        
        # [2] ì†ì„± ë¶„ì„
        spec_list = []
        for spec in self.df['ìŠ¤í™'].dropna():
            if spec != '-':
                parts = [p.strip() for p in str(spec).split('|')]
                spec_list.extend([p for p in parts if len(p) > 1 and p not in self.exclude_brands])
        spec_counts = Counter(spec_list).most_common(8)

        # [3] íƒœê·¸ ë¶„ì„ - NLU Term ê¸°ë°˜ ì¤‘ë³µ ì œê±° ë° ì¡°í•© í™•ì¥
        tag_raw_list = []
        for tags_row in self.df['ê²€ìƒ‰ì¸ì‹íƒœê·¸'].dropna():
            if tags_row != '-':
                tag_raw_list.extend([t.strip() for t in str(tags_row).split(',')])
        
        # ì œëª©ì— ì´ë¯¸ í¬í•¨ëœ ì˜ë¯¸(Term) ì§‘í•© êµ¬ì„±
        title_terms = manual_terms.copy()
        for word, _ in readable_auto:
            title_terms.update(self.extract_nlu_terms(word))

        # í›„ë³´ íƒœê·¸ ì ìˆ˜í™” (ë¹ˆë„ìˆ˜ + ì˜ë¯¸ ë‹¤ì–‘ì„±)
        candidate_tags = []
        tag_freq = Counter(tag_raw_list).most_common(200)
        
        for t, c in tag_freq:
            # ê¸°ë³¸ í•„í„°: ë¸Œëœë“œ ì œì™¸, ìˆ˜ì¹˜ ì œì™¸
            if any(b in t for b in self.exclude_brands) or any(char.isdigit() for char in t):
                continue
            
            tag_terms = self.extract_nlu_terms(t)
            # ì œëª©ê³¼ ì˜ë¯¸ê°€ ì™„ì „íˆ ê²¹ì¹˜ëŠ” íƒœê·¸ ì œì™¸
            if tag_terms.issubset(title_terms):
                continue
            
            candidate_tags.append({'tag': t, 'count': c, 'terms': tag_terms})

        # ìµœì¢… íƒœê·¸ ì„ ë³„ ë¡œì§ (ì˜ë¯¸ ì¤‘ë³µ ë°°ì œ ë° í™•ì¥ ê·¹ëŒ€í™”)
        final_tags = []
        selected_terms_pool = title_terms.copy()

        # 1ì°¨: ê°€ì¥ 'ì •ë³´ëŸ‰ì´ ë§ì€(Termì´ ë§ì€)' íƒœê·¸ë¶€í„° ê²€í† í•˜ì—¬ ë‹¤ì–‘ì„± í™•ë³´
        # ë¹ˆë„ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ë˜ ì˜ë¯¸ ì¤‘ë³µì„ ì²´í¬í•¨
        sorted_candidates = sorted(candidate_tags, key=lambda x: x['count'], reverse=True)

        for cand in sorted_candidates:
            if len(final_tags) >= 10: break
            
            # í˜„ì¬ íƒœê·¸ì˜ ì˜ë¯¸ë“¤ì´ ì´ë¯¸ ì„ íƒëœ ì˜ë¯¸ í’€(Pool)ê³¼ 80% ì´ìƒ ê²¹ì¹˜ë©´ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨
            # ì˜ˆ: 'ì œê³¼ì œë¹µì¬ë£Œ'ê°€ ë“¤ì–´ìˆëŠ”ë° 'ì œê³¼ìš©'ì´ ë“¤ì–´ì˜¤ëŠ” ê²ƒì„ ë°©ì§€
            overlap = cand['terms'].intersection(selected_terms_pool)
            
            if len(cand['terms']) > 0 and (len(overlap) / len(cand['terms'])) < 0.6:
                final_tags.append((cand['tag'], cand['count']))
                selected_terms_pool.update(cand['terms'])

        return manual_keywords, readable_auto, spec_counts, final_tags

# 3. ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ (GUI)
st.sidebar.header("ğŸ“ ë°ì´í„° ì—…ë¡œë“œ")
uploaded_file = st.sidebar.file_uploader("ë¶„ì„ìš© CSV íŒŒì¼ ì—…ë¡œë“œ", type=["csv"])

st.sidebar.header("ğŸ¯ ì „ëµ í‚¤ì›Œë“œ")
manual_input = st.sidebar.text_input("êµ¬ë§¤ ìœ ì… í‚¤ì›Œë“œ ì…ë ¥", placeholder="ì˜ˆ: ì†í¸í•œ êµ­ë‚´ì‚°")

if uploaded_file:
    try:
        df = pd.read_csv(uploaded_file, encoding='cp949')
    except:
        uploaded_file.seek(0)
        df = pd.read_csv(uploaded_file, encoding='utf-8-sig')

    manager = SEOManager(df)
    manual_keys, auto_keys, specs, tags = manager.run_analysis(manual_input)

    st.success("âœ… NLU Term ë¶„ì„ì„ í†µí•œ í‚¤ì›Œë“œ ìµœì í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    # ì„¹ì…˜ 1: ìƒí’ˆëª…
    st.header("ğŸ·ï¸ 1. ì „ëµì  ìƒí’ˆëª… ì¡°í•©")
    col1, col2 = st.columns([2, 1])
    with col1:
        full_title = " ".join(manual_keys + [p[0] for p in auto_keys])
        st.code(full_title, language=None)
        st.info("**NLU ì „ëµ:** ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„ì„í•˜ì—¬ ê°€ë…ì„±ì´ ê°€ì¥ ë†’ì€ ìˆœì„œë¡œ ë°°ì—´í–ˆìŠµë‹ˆë‹¤.")
    with col2:
        st.table(pd.DataFrame(auto_keys, columns=['ë‹¨ì–´', 'ë¹ˆë„']))

    st.markdown("---")

    # ì„¹ì…˜ 2: ì†ì„±
    st.header("âš™ï¸ 2. ê¶Œì¥ ì†ì„±ê°’")
    col3, col4 = st.columns([2, 1])
    with col3:
        for s, _ in specs: st.button(s, use_container_width=True)
    with col4:
        st.table(pd.DataFrame(specs, columns=['ì†ì„±ê°’', 'ë¹ˆë„']))

    st.markdown("---")

    # ì„¹ì…˜ 3: íƒœê·¸ (NLU í™•ì¥ ë¡œì§ ì ìš©)
    st.header("ğŸ” 3. í™•ì¥ ê²€ìƒ‰ íƒœê·¸ (ì˜ë¯¸ ê¸°ë°˜ ì¤‘ë³µ ì œê±°)")
    col5, col6 = st.columns([2, 1])
    with col5:
        st.warning(", ".join([f"#{t[0]}" for t in tags]))
        st.info("""
        **NLU Term í•„í„°ë§ ì ìš©:**
        - '#ì œê³¼ì œë¹µì¬ë£Œ'ê°€ ì„ ì •ë˜ë©´ ìœ ì‚¬ ì˜ë¯¸ì¸ '#ì œê³¼ìš©', '#ì œê³¼ì œë¹µìš©í’ˆ'ì€ ìë™ìœ¼ë¡œ ë°°ì œë©ë‹ˆë‹¤.
        - ë‚¨ì€ ìë¦¬ì— ìƒˆë¡œìš´ ìœ ì… ê²½ë¡œ(ë§›, ì˜ì–‘, ìš©ë„ ë“±)ë¥¼ ê°€ì§„ í‚¤ì›Œë“œë¥¼ ìš°ì„  ë°°ì¹˜í•˜ì—¬ **ê²€ìƒ‰ ê²½ìš°ì˜ ìˆ˜**ë¥¼ ê·¹ëŒ€í™”í–ˆìŠµë‹ˆë‹¤.
        """)
    with col6:
        st.table(pd.DataFrame(tags, columns=['íƒœê·¸ëª…', 'ì¸ì‹ íšŸìˆ˜']))

else:
    st.info("CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ NLU ê¸°ë°˜ ë¶„ì„ì´ ì‹œì‘ë©ë‹ˆë‹¤.")
