import streamlit as st
import pandas as pd
import re
from collections import Counter

# 1. í˜ì´ì§€ ì„¤ì • ë° ë””ìì¸
st.set_page_config(page_title="ë„¤ì´ë²„ SEO NLU ë§ˆìŠ¤í„°", layout="wide")
st.title("ğŸš€ ë„¤ì´ë²„ ì‡¼í•‘ SEO í†µí•© ìµœì í™” ë§¤ë‹ˆì €")
st.markdown("---")

class SEOManager:
    def __init__(self, df, user_exclude_list):
        self.df = df
        self.exclude_brands = set([
            'ë§¤ì¼', 'ì„œìš¸ìš°ìœ ', 'ì„œìš¸', 'ì—°ì„¸', 'ë‚¨ì–‘', 'ê±´êµ­', 'íŒŒìŠ¤í‡´ë¥´', 'ì¼ë™', 'í›„ë””ìŠ¤', 
            'ì†Œì™€ë‚˜ë¬´', 'ë¹™ê·¸ë ˆ', 'ì…€ë¡œëª¬', 'ë¹…ì›ë”', 'ë¯¸ê´‘ìŠ¤í† ì–´', 'ë°ì–´ë¦¬ë§ˆì¼“', 'ë„ë‚¨ìƒíšŒ', 
            'í¬ì°½ìœ ì—…', 'ë‹´í„°', 'ì—°ì„¸ìœ ì—…', 'ë§¤ì¼ìœ ì—…'
        ] + user_exclude_list)
        # NLU ë¶„ë¦¬ ê¸°ì¤€ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ (ê¸´ ë‹¨ì–´ë¶€í„° ë§¤ì¹­)
        self.sub_splits = sorted([
            'ìíŒê¸°', 'ìš°ìœ ', 'ë¶„ìœ ', 'ê°€ë£¨', 'ë¶„ë§', 'ì „ì§€', 'íƒˆì§€', 'ìŠ¤í‹±', 
            'ì—…ì†Œìš©', 'ëŒ€ìš©ëŸ‰', 'ì‹ìì¬', 'ì œê³¼', 'ì œë¹µ', 'ë² ì´í‚¹', 'ë©¸ê· ', 'íŒŒìš°ì¹˜'
        ], key=len, reverse=True)

    def split_base_terms(self, text):
        """ë¶™ì—¬ ì“´ í‚¤ì›Œë“œë„ NLU ê·œì¹™ì— ë”°ë¼ ìë™ìœ¼ë¡œ ìª¼ê°œì£¼ëŠ” ì—”ì§„"""
        if pd.isna(text) or text == '-': return []
        text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', ' ', str(text))
        
        # íŒ¨í„´ ê¸°ë°˜ ê°•ì œ ë¶„ë¦¬ (ì˜ˆ: ë§›ìˆëŠ”ìíŒê¸°ìš°ìœ  -> ë§›ìˆëŠ” ìíŒê¸° ìš°ìœ )
        pattern = f"({'|'.join(self.sub_splits)})"
        raw_parts = re.split(pattern, text)
        
        terms = []
        for part in raw_parts:
            p = part.strip()
            if not p or p in self.exclude_brands or any(c.isdigit() for c in p): continue
            if len(p) > 1 or p in self.sub_splits:
                terms.extend(p.split())
        return terms

    def reorder_for_readability(self, word_count_pairs):
        identity, form, usage, desc = ['ì „ì§€', 'ë¶„ìœ ', 'ìš°ìœ ', 'íƒˆì§€'], ['ë¶„ë§', 'ê°€ë£¨', 'ìŠ¤í‹±', 'ì•¡ìƒ'], ['ìíŒê¸°', 'ì—…ì†Œìš©', 'ëŒ€ìš©ëŸ‰', 'ì‹ìì¬'], ['ì§„í•œ', 'ê³ ì†Œí•œ', 'ë§›ìˆëŠ”', 'ì¶”ì–µ']
        def get_priority(pair):
            w = pair[0]
            if any(c in w for c in identity): return 1
            if any(c in w for c in form): return 2
            if any(c in w for c in usage): return 3
            if any(c in w for c in desc): return 4
            return 5
        return sorted(word_count_pairs, key=lambda x: get_priority(x))

    def run_analysis(self, conv_input, add_input, total_count):
        # ìˆ˜ë™ ì…ë ¥ í‚¤ì›Œë“œë„ NLU ë¶„ë¦¬ ì ìš©
        conv_keys = self.split_base_terms(conv_input)
        add_keys = self.split_base_terms(add_input)
        fixed_keywords = conv_keys + add_keys
        
        name_terms = []
        for name in self.df['ìƒí’ˆëª…']: name_terms.extend(self.split_base_terms(name))
        name_freq = Counter(name_terms).most_common(50)
        
        auto_candidates = [(w, c) for w, c in name_freq if w not in fixed_keywords]
        readable_auto = self.reorder_for_readability(auto_candidates[:max(0, total_count - len(fixed_keywords))])
        
        # [ì´ë¯¸ì§€ 77c3cc ì¬í˜„] ìŠ¤í™ ë¶„ì„
        spec_list = []
        for spec in self.df['ìŠ¤í™'].dropna():
            parts = [p.strip() for p in str(spec).split('|') if len(p.strip()) > 1]
            spec_list.extend([p for p in parts if p not in self.exclude_brands])
        specs = Counter(spec_list).most_common(8)

        # [ì´ë¯¸ì§€ 77c3cc ì¬í˜„] íƒœê·¸ ë¶„ì„ (ìˆ˜ì‹ì–´ ì¤‘ë³µ ë°°ì œ)
        tag_raw = []
        for row in self.df['ê²€ìƒ‰ì¸ì‹íƒœê·¸'].dropna():
            tag_raw.extend([t.strip() for t in str(row).split(',') if t.strip()])
        
        tag_freq_map = Counter(tag_raw)
        title_set = set(fixed_keywords + [p[0] for p in readable_auto])
        candidates = [(t, c) for t, c in tag_freq_map.most_common(150) if t not in title_set and not any(c.isdigit() for c in t)]

        final_tags = []
        used_prefixes = set()
        for t, c in candidates:
            if len(final_tags) >= 10: break
            prefix = t[:3] if len(t) > 3 else t[:2]
            if not any(prefix in ex_t or ex_t[:3] in t for ex_t, _ in final_tags):
                final_tags.append((t, c))
        
        return conv_keys, add_keys, readable_auto, specs, final_tags

def check_metrics(text):
    c_len = len(text)
    try: b_len = len(text.encode('euc-kr'))
    except: b_len = len(text.encode('utf-8'))
    return c_len, b_len

# 3. GUI êµ¬ì„±
st.sidebar.header("ğŸ¯ ì„¤ì •")
uploaded_file = st.sidebar.file_uploader("CSV ì—…ë¡œë“œ", type=["csv"])
conv_in = st.sidebar.text_input("êµ¬ë§¤ì „í™˜ í‚¤ì›Œë“œ", placeholder="ë§›ìˆëŠ”ìíŒê¸°ìš°ìœ ")
add_in = st.sidebar.text_input("ì¶”ê°€ í‚¤ì›Œë“œ")
total_kw = st.sidebar.number_input("ëª©í‘œ í‚¤ì›Œë“œ ìˆ˜", value=11)

if uploaded_file:
    try: df = pd.read_csv(uploaded_file, encoding='cp949')
    except: df = pd.read_csv(uploaded_file, encoding='utf-8-sig')

    manager = SEOManager(df, [])
    conv, add, auto, specs, tags = manager.run_analysis(conv_in, add_in, total_kw)

    # ì„¹ì…˜ 1: ìƒí’ˆëª…
    st.header("ğŸ·ï¸ 1. ì „ëµì  ìƒí’ˆëª… ì¡°í•©")
    full_title = " ".join(conv + add + [p[0] for p in auto])
    st.code(full_title, language=None)
    
    c_l, b_l = check_metrics(full_title)
    kw_c = len(conv) + len(add) + len(auto)
    st.markdown(f"{'ğŸŸ¢ ì •ìƒ' if c_l <= 50 else 'ğŸ”´ ì£¼ì˜'}: **{c_l}ì / {b_l} Byte / {kw_c}ê°œ í‚¤ì›Œë“œ**")
    
    st.markdown("---")

    # [ì´ë¯¸ì§€ 77c3cc ë ˆì´ì•„ì›ƒ ì¬í˜„] ì„¹ì…˜ 2 & 3 í†µí•© ì¶œë ¥
    st.header("âš™ï¸ 2. í•„í„° ì†ì„± & ğŸ” 3. í™•ì¥ íƒœê·¸")
    col_left, col_right = st.columns([1, 1])

    with col_left:
        # ì™¼ìª½: í•„í„° ì†ì„± ë²„íŠ¼ ë¦¬ìŠ¤íŠ¸
        for s_name, _ in specs:
            st.button(s_name, use_container_width=True, key=f"btn_{s_name}")

    with col_right:
        # ì˜¤ë¥¸ìª½: ì—°ë¡ìƒ‰ ë°•ìŠ¤ ë‚´ íƒœê·¸ ë¦¬ìŠ¤íŠ¸
        tag_string = ", ".join([f"#{t[0]}" for t in tags])
        st.success(tag_string)

else:
    st.info("ì‚¬ì´ë“œë°”ì—ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
