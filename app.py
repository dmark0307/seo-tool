import streamlit as st
import pandas as pd
import re
from collections import Counter

# 1. í˜ì´ì§€ ì„¤ì • ë° ë””ìì¸
st.set_page_config(page_title="ë„¤ì´ë²„ SEO NLU ë§ˆìŠ¤í„°", layout="wide")
st.title("ğŸš€ ë„¤ì´ë²„ ì‡¼í•‘ SEO í†µí•© ìµœì í™” ë§¤ë‹ˆì €")
st.markdown("---")

# 2. ì „ë¬¸ SEO ë¶„ì„ ë¡œì§ í´ë˜ìŠ¤
class SEOManager:
    def __init__(self, df, user_exclude_list):
        self.df = df
        self.exclude_brands = [
            'ë§¤ì¼', 'ì„œìš¸ìš°ìœ ', 'ì„œìš¸', 'ì—°ì„¸', 'ë‚¨ì–‘', 'ê±´êµ­', 'íŒŒìŠ¤í‡´ë¥´', 'ì¼ë™', 'í›„ë””ìŠ¤', 
            'ì†Œì™€ë‚˜ë¬´', 'ë¹™ê·¸ë ˆ', 'ì…€ë¡œëª¬', 'ë¹…ì›ë”', 'ë¯¸ê´‘ìŠ¤í† ì–´', 'ë°ì–´ë¦¬ë§ˆì¼“', 'ë„ë‚¨ìƒíšŒ', 
            'í¬ì°½ìœ ì—…', 'ë‹´í„°', 'ì—°ì„¸ìœ ì—…', 'ë§¤ì¼ìœ ì—…'
        ] + user_exclude_list
        # NLU ë¶„ë¦¬ ê¸°ì¤€ í•µì‹¬ ë‹¨ì–´ (í™•ì¥ì„± ì²´í¬ë¥¼ ìœ„í•´ 'ì¶”ì–µ', 'ê°„ì‹' ë“± ì¶”ê°€)
        self.sub_splits = sorted(['ìíŒê¸°', 'ìš°ìœ ', 'ë¶„ìœ ', 'ê°€ë£¨', 'ë¶„ë§', 'ì „ì§€', 'íƒˆì§€', 'ìŠ¤í‹±', 'ì—…ì†Œìš©', 'ëŒ€ìš©ëŸ‰', 'ë©¸ê· ', 'íŒŒìš°ì¹˜', 'ì¶”ì–µ', 'ê°„ì‹', 'ì¬ë£Œ'], key=len, reverse=True)

    def split_base_terms(self, text):
        """NLU ê·œì¹™ì— ë”°ë¼ ë³µí•© ëª…ì‚¬ë¥¼ ìë™ìœ¼ë¡œ ë¶„ë¦¬í•˜ëŠ” ì—”ì§„"""
        if pd.isna(text) or text == '-': return []
        text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', ' ', str(text))
        raw_words = text.split()
        
        terms = []
        pattern = f"({'|'.join(self.sub_splits)})"
        
        for word in raw_words:
            if word in self.exclude_brands or any(char.isdigit() for char in word):
                continue
            parts = re.split(pattern, word)
            for p in parts:
                p = p.strip()
                if not p or p in self.exclude_brands: continue
                if len(p) > 1 or p in self.sub_splits:
                    terms.append(p)
        return terms

    def reorder_for_readability(self, word_count_pairs):
        identity, form, usage, desc = ['ì „ì§€', 'ë¶„ìœ ', 'ìš°ìœ ', 'íƒˆì§€'], ['ë¶„ë§', 'ê°€ë£¨', 'ìŠ¤í‹±', 'ì•¡ìƒ'], ['ìíŒê¸°', 'ì—…ì†Œìš©', 'ëŒ€ìš©ëŸ‰', 'ì‹ìì¬'], ['ì§„í•œ', 'ê³ ì†Œí•œ', 'ë§›ìˆëŠ”', 'ì¶”ì–µ']
        def get_priority(pair):
            word = pair[0]
            if any(core in word for core in identity): return 1
            if any(core in word for core in form): return 2
            if any(core in word for core in usage): return 3
            if any(core in word for core in desc): return 4
            return 5
        return sorted(word_count_pairs, key=lambda x: get_priority(x))

    def run_analysis(self, conversion_input, add_input, total_target_count):
        # 1. ê³ ì • í‚¤ì›Œë“œ (NLU ë¶„ë¦¬ ì ìš©)
        conv_keys = self.split_base_terms(conversion_input)
        add_keys = self.split_base_terms(add_input)
        fixed_keywords = conv_keys + add_keys
        
        # 2. ìƒí’ˆëª… ìë™ ì¶”ì¶œ í‚¤ì›Œë“œ
        name_terms = []
        for name in self.df['ìƒí’ˆëª…']:
            name_terms.extend(self.split_base_terms(name))
        
        name_freq = Counter(name_terms).most_common(50)
        auto_candidates = [w for w, c in name_freq if w not in fixed_keywords]
        
        remain_count = max(0, total_target_count - len(fixed_keywords))
        selected_auto = auto_candidates[:remain_count]
        readable_auto_pairs = self.reorder_for_readability([(w, Counter(name_terms)[w]) for w in selected_auto])
        
        # 3. ì†ì„±(ìŠ¤í™) ë¶„ì„ (ë¡œì§ ë° ì¶œë ¥ ìœ ì§€)
        spec_list = []
        for spec in self.df['ìŠ¤í™'].dropna():
            if spec != '-':
                parts = [p.strip() for p in str(spec).split('|')]
                spec_list.extend([p for p in parts if len(p) > 1 and p not in self.exclude_brands])
        spec_counts = Counter(spec_list).most_common(8)
        # ì†ì„±ì—ì„œ ì‚¬ìš©ëœ ê°œë³„ í‚¤ì›Œë“œ ì¶”ì¶œ
        spec_keywords = set()
        for s, _ in spec_counts:
            spec_keywords.update(self.split_base_terms(s))

        # 4. ìƒí’ˆëª…ì— ì‚¬ìš©ëœ ì „ì²´ í‚¤ì›Œë“œ ì§‘í•©
        title_keywords = set(fixed_keywords + [p[0] for p in readable_auto_pairs])

        # 5. í™•ì¥ ê²€ìƒ‰ íƒœê·¸ ë¶„ì„ (í™•ì¥ì„± ê·¹ëŒ€í™” ì—…ê·¸ë ˆì´ë“œ)
        tag_raw_list = []
        for tags in self.df['ê²€ìƒ‰ì¸ì‹íƒœê·¸'].dropna():
            if tags != '-':
                tag_raw_list.extend([t.strip() for t in str(tags).split(',') if t.strip()])
        
        tag_freq = Counter(tag_raw_list).most_common(300)
        
        final_tags = []
        # ì´ë¯¸ ì„ ì ëœ íƒœê·¸ì˜ 'ì˜ë¯¸ì  ì¡°ê°'ë“¤ (ì¤‘ë³µ ì°¨ë‹¨ìš©)
        master_keyword_pool = title_keywords.union(spec_keywords)

        for t_raw, c in tag_freq:
            if len(final_tags) >= 10: break
            
            # íƒœê·¸ë¥¼ NLU ë‹¨ìœ„ë¡œ ë¶„í•´
            t_subterms = self.split_base_terms(t_raw)
            if not t_subterms:
                if len(t_raw) > 1: t_subterms = [t_raw]
                else: continue
            
            # [ì§€ëŠ¥í˜• ì¤‘ë³µ ê²€ì‚¬]
            is_redundant = False
            
            # (1) ê¸°ì¡´ í‚¤ì›Œë“œ í’€(ìƒí’ˆëª…/ì†ì„±/ê¸°ì„ íƒ íƒœê·¸)ê³¼ ì¡°ê° ë‹¨ì–´ê°€ ê²¹ì¹˜ëŠ”ì§€ ì²´í¬
            for sub in t_subterms:
                if sub in master_keyword_pool:
                    is_redundant = True; break
            
            # (2) ë¬¸ìì—´ í¬í•¨ ê´€ê³„ ì²´í¬ (ì˜ˆ: #ì œê³¼ì œë¹µ vs #ì œê³¼ì œë¹µì¬ë£Œ)
            if not is_redundant:
                for existing_t, _ in final_tags:
                    if t_raw in existing_t or existing_t in t_raw:
                        is_redundant = True; break

            # (3) ìˆ«ì í¬í•¨ ì œì™¸
            if any(char.isdigit() for char in t_raw): is_redundant = True

            if not is_redundant:
                final_tags.append((t_raw, c))
                # ë½‘íŒ íƒœê·¸ì˜ ëª¨ë“  ì¡°ê° ë‹¨ì–´ë¥¼ í’€ì— ì¶”ê°€í•˜ì—¬ ì´í›„ ìœ ì‚¬ ë‹¨ì–´ ì°¨ë‹¨
                for sub in t_subterms: master_keyword_pool.add(sub)

        return fixed_keywords, readable_auto_pairs, spec_counts, sorted(final_tags, key=lambda x: x[1], reverse=True)[:10]

def calculate_seo_metrics(text):
    char_count = len(text)
    try: byte_count = len(text.encode('euc-kr'))
    except: byte_count = len(text.encode('utf-8'))
    return char_count, byte_count

# 3. GUI êµ¬ì„± (ë ˆì´ì•„ì›ƒ ë³€ë™ ì—†ìŒ)
st.sidebar.header("ğŸ“ Step 1. ë°ì´í„° ì—…ë¡œë“œ")
uploaded_file = st.sidebar.file_uploader("ë¶„ì„ìš© CSV íŒŒì¼ ì—…ë¡œë“œ", type=["csv"])

st.sidebar.header("ğŸ¯ Step 2. ì „ëµ ì„¤ì •")
conversion_input = st.sidebar.text_input("êµ¬ë§¤ì „í™˜ í‚¤ì›Œë“œ", placeholder="ì˜ˆ: ë§›ìˆëŠ”ìíŒê¸°ìš°ìœ ")
add_input = st.sidebar.text_input("ì¶”ê°€í•  í‚¤ì›Œë“œ", placeholder="ì˜ˆ: ë¬´ë£Œë°°ì†¡ë‹¹ì¼ë°œì†¡")
total_kw_count = st.sidebar.number_input("ìƒí’ˆëª… ëª©í‘œ í‚¤ì›Œë“œ ìˆ˜", min_value=5, max_value=25, value=11)

if uploaded_file:
    try: df = pd.read_csv(uploaded_file, encoding='cp949')
    except:
        uploaded_file.seek(0)
        df = pd.read_csv(uploaded_file, encoding='utf-8-sig')

    manager = SEOManager(df, [])
    fixed, auto, specs, tags = manager.run_analysis(conversion_input, add_input, total_kw_count)

    st.success("âœ¨ NLU ê¸°ë°˜ ì •ë°€ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    # ì„¹ì…˜ 1: ìƒí’ˆëª… (ë³€ë™ ì—†ìŒ)
    st.header("ğŸ·ï¸ 1. ì „ëµì  ìƒí’ˆëª… ì¡°í•©")
    col1, col2 = st.columns([2, 1])
    with col1:
        st.subheader("âœ… ì™„ì„±ëœ ìƒí’ˆëª…")
        full_title = " ".join(fixed + [p[0] for p in auto])
        st.code(full_title, language=None)
        c_len, b_len = calculate_seo_metrics(full_title)
        st.markdown(f"{'ğŸŸ¢ ì •ìƒ' if c_len <= 50 else 'ğŸ”´ ì£¼ì˜'}: {c_len}ì / {b_len} Byte / {len(fixed)+len(auto)}ê°œ í‚¤ì›Œë“œ")

    with col2:
        st.subheader("ğŸ“Š ìë™ ì¶”ì²œ í‚¤ì›Œë“œ")
        st.table(pd.DataFrame(auto, columns=['ë‹¨ì–´', 'ë¹ˆë„']).assign(No=range(1, len(auto)+1)).set_index('No'))

    st.markdown("---")

    # ì„¹ì…˜ 2: ì†ì„± (ë¡œì§ ë° ì¶œë ¥ ìœ ì§€)
    st.header("âš™ï¸ 2. í•„í„° ë…¸ì¶œìš© ì†ì„±ê°’")
    col3, col4 = st.columns([2, 1])
    with col3:
        for s, _ in specs: st.button(s, key=f"attr_{s}", use_container_width=True)
    with col4:
        st.table(pd.DataFrame(specs, columns=['ì†ì„±ê°’', 'ë¹ˆë„']).set_index(pd.Index(range(1, len(specs)+1))))

    st.markdown("---")

    # ì„¹ì…˜ 3: íƒœê·¸ (ì—…ê·¸ë ˆì´ë“œëœ ë¡œì§ ì ìš©)
    st.header("ğŸ” 3. í™•ì¥ ê²€ìƒ‰ íƒœê·¸ (ì¡°í•© íš¨ìœ¨ ê·¹ëŒ€í™”)")
    col5, col6 = st.columns([2, 1])
    with col5:
        st.subheader("âœ… ìµœì í™” íƒœê·¸ 10ì„ ")
        tag_display = ", ".join([f"#{t[0]}" for t in tags])
        st.success(tag_display)
        st.caption("â€» íƒœê·¸ë¥¼ NLU ë‹¨ìœ„ë¡œ ë¶„í•´í•˜ê³  ë¬¸ìì—´ í¬í•¨ ê´€ê³„ë¥¼ ë¶„ì„í•˜ì—¬ ìœ ì… ê²½ë¡œê°€ ê²¹ì¹˜ì§€ ì•ŠëŠ” ìµœì ì˜ ì¡°í•©ì„ ì„ ë³„í–ˆìŠµë‹ˆë‹¤.")
    with col6:
        st.subheader("ğŸ“Š íƒœê·¸ ì‚¬ìš© ë¹ˆë„ìˆ˜")
        tag_df = pd.DataFrame(tags, columns=['íƒœê·¸ëª…', 'ì‚¬ìš© ë¹ˆë„ìˆ˜'])
        tag_df.index += 1
        st.table(tag_df)
else:
    st.info("íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
