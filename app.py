import streamlit as st
import pandas as pd
import re
from collections import Counter
import io

# 1. í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ë„¤ì´ë²„ SEO NLU ë§ˆìŠ¤í„°", layout="wide")
st.title("ğŸš€ ë„¤ì´ë²„ ì‡¼í•‘ SEO í†µí•© ìµœì í™” (Pro ë²„ì „)")
st.markdown("---")

# 2. ì „ë¬¸ SEO ë¶„ì„ ë¡œì§ í´ë˜ìŠ¤
class SEOManager:
    def __init__(self, df, user_exclude_list):
        self.df = df
        # ë¸Œëœë“œ ë° ì œì™¸ í‚¤ì›Œë“œ í†µí•©
        self.exclude_brands = set([
            'ë§¤ì¼', 'ì„œìš¸ìš°ìœ ', 'ì„œìš¸', 'ì—°ì„¸', 'ë‚¨ì–‘', 'ê±´êµ­', 'íŒŒìŠ¤í‡´ë¥´', 'ì¼ë™', 'í›„ë””ìŠ¤', 
            'ì†Œì™€ë‚˜ë¬´', 'ë¹™ê·¸ë ˆ', 'ì…€ë¡œëª¬', 'ë¹…ì›ë”', 'ë¯¸ê´‘ìŠ¤í† ì–´', 'ë°ì–´ë¦¬ë§ˆì¼“', 'ë„ë‚¨ìƒíšŒ', 
            'í¬ì°½ìœ ì—…', 'ë‹´í„°', 'ì—°ì„¸ìœ ì—…', 'ë§¤ì¼ìœ ì—…'
        ] + user_exclude_list)

        # NLU ë¶„ì„ì„ ìœ„í•œ í˜•íƒœì†Œ ë¶„ë¦¬ ê¸°ì¤€
        self.sub_splits = ['ìíŒê¸°', 'ìš°ìœ ', 'ë¶„ìœ ', 'ê°€ë£¨', 'ë¶„ë§', 'ì „ì§€', 'íƒˆì§€', 'ìŠ¤í‹±', 'ì—…ì†Œìš©', 'ëŒ€ìš©ëŸ‰']

    def normalize_text(self, text):
        """í…ìŠ¤íŠ¸ ì •ê·œí™”: íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ê³µë°± ì •ë¦¬"""
        if pd.isna(text): return ""
        # ì œì–´ ë¬¸ì ì œê±° ë° í•œê¸€/ì˜ë¬¸/ìˆ«ìë§Œ ë‚¨ê¸°ê¸°
        text = re.sub(r'[\x00-\x1F\x7F]', '', str(text))
        text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', ' ', text)
        return text.strip()

    def split_base_terms(self, text):
        """ìƒí’ˆëª… ë¶„ì„: ìˆ˜ì¹˜/ë¸Œëœë“œ/ë¶ˆìš©ì–´ ì œê±° í›„ ìœ íš¨ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        text = self.normalize_text(text)
        raw_words = text.split()
        terms = []
        
        for word in raw_words:
            if word in self.exclude_brands or any(char.isdigit() for char in word):
                continue
            
            found_sub = False
            for sub in self.sub_splits:
                if sub in word and word != sub:
                    terms.append(sub)
                    rem = word.replace(sub, '').strip()
                    if len(rem) > 1 and not any(char.isdigit() for char in rem) and rem not in self.exclude_brands:
                        terms.append(rem)
                    found_sub = True
                    break
            if not found_sub and len(word) > 1:
                terms.append(word)
        return terms

    def reorder_for_readability(self, word_count_pairs):
        """ìƒí’ˆëª… ê°€ë…ì„± ë°°ì¹˜ ì „ëµ"""
        identity = ['ì „ì§€', 'ë¶„ìœ ', 'ìš°ìœ ', 'íƒˆì§€', 'ì „ì§€ë°€']
        form = ['ë¶„ë§', 'ê°€ë£¨', 'ìŠ¤í‹±', 'ì•¡ìƒ']
        usage = ['ìíŒê¸°', 'ì—…ì†Œìš©', 'ëŒ€ìš©ëŸ‰', 'ì‹ìì¬', 'ì œê³¼', 'ì œë¹µ', 'ë² ì´í‚¹']
        desc = ['ì§„í•œ', 'ê³ ì†Œí•œ', 'ë§›ìˆëŠ”', 'ì¶”ì–µ', 'ì¶”ì²œ', 'ì†í¸í•œ']

        def get_priority(pair):
            word = pair[0]
            if any(core in word for core in identity): return 1
            if any(core in word for core in form): return 2
            if any(core in word for core in usage): return 3
            if any(core in word for core in desc): return 4
            return 5
        return sorted(word_count_pairs, key=lambda x: get_priority(x))

    def run_analysis(self, conversion_input, add_input, total_target_count):
        # [1] ê³ ì • í‚¤ì›Œë“œ ì²˜ë¦¬
        conv_keys = [w.strip() for w in conversion_input.split() if w.strip()]
        add_keys = [w.strip() for w in add_input.split() if w.strip()]
        fixed_keywords = conv_keys + add_keys
        
        # [2] ìƒí’ˆëª… ìë™ ì¶”ì¶œ (Vectorized-like processing)
        all_name_words = []
        # applyë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°í”„ë ˆì„ ì²˜ë¦¬ ì†ë„ ìµœì í™”
        processed_names = self.df['ìƒí’ˆëª…'].apply(self.split_base_terms)
        for words in processed_names:
            all_name_words.extend(words)
            
        name_counts = Counter(all_name_words)
        # ê³ ì • í‚¤ì›Œë“œì™€ 'ì™„ì „ ì¼ì¹˜'í•˜ëŠ” ë‹¨ì–´ë§Œ ì œì™¸
        auto_candidates = [(w, c) for w, c in name_counts.most_common(100) if w not in fixed_keywords]
        
        remain_count = max(0, total_target_count - len(fixed_keywords))
        selected_auto_pairs = auto_candidates[:remain_count]
        readable_auto_pairs = self.reorder_for_readability(selected_auto_pairs)
        
        # [3] ì†ì„± ë¶„ì„
        spec_list = []
        spec_series = self.df['ìŠ¤í™'].dropna().astype(str)
        for spec in spec_series:
            if spec != '-':
                parts = [p.strip() for p in spec.split('|')]
                spec_list.extend([p for p in parts if len(p) > 1 and p not in self.exclude_brands])
        spec_counts = Counter(spec_list).most_common(8)

        # [4] íƒœê·¸ ë¶„ì„ (ì •ë°€ ì¹´ìš´íŒ… + í™•ì¥ ì¶”ì²œ)
        tag_series = self.df['ê²€ìƒ‰ì¸ì‹íƒœê·¸'].dropna().astype(str)
        
        # A. ì›ë³¸ ë°ì´í„° ì •ë°€ ì¹´ìš´íŒ… (í†µê³„ìš©)
        raw_tags_all = []
        for row in tag_series:
            if row != '-':
                # ê³µë°±/íŠ¹ìˆ˜ë¬¸ì ì œê±° í›„ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                tags = [t.strip() for t in row.split(',') if t.strip()]
                raw_tags_all.extend(tags)
        
        raw_tag_stats = Counter(raw_tags_all).most_common(50) # ì—‘ì…€ ì›ë³¸ í†µê³„

        # B. ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ (ì¶”ì²œìš©)
        # ì œëª©ì— í¬í•¨ëœ ë‹¨ì–´ ì§‘í•© (ì™„ì „ ì¼ì¹˜ ë¹„êµìš©)
        title_set = set(fixed_keywords + [p[0] for p in readable_auto_pairs])
        
        valid_candidates = []
        for t, c in Counter(raw_tags_all).most_common(300):
            # ë¸Œëœë“œ/ìˆ«ì í•„í„°ë§
            if any(b in t for b in self.exclude_brands) or any(char.isdigit() for char in t):
                continue
            # ì œëª©ì— ì´ë¯¸ ìˆëŠ” ë‹¨ì–´ëŠ” 'ì¶”ì²œ'ì—ì„œë§Œ ì œì™¸ (í†µê³„ì—ëŠ” ë‚¨ìŒ)
            if t in title_set:
                continue
            valid_candidates.append((t, c))

        # C. ì¡°í•© í™•ì¥ì„± ë¡œì§ (í´ëŸ¬ìŠ¤í„°ë§)
        final_tags = []
        clusters = {
            'ì œê³¼': ['ì œê³¼', 'ì œë¹µ', 'ë² ì´í‚¹'], 
            'ë§›': ['ë§›', 'ë‹¬ë‹¬', 'ê³ ì†Œ', 'í’ë¯¸'], 
            'ì˜ì–‘': ['ì˜ì–‘', 'ë‹¨ë°±ì§ˆ', 'ê±´ê°•'], 
            'ìš©ë„': ['ìíŒê¸°', 'ì‹ìì¬', 'ìš”ë¦¬']
        }
        used_roots = set()

        # C-1. ì¹´í…Œê³ ë¦¬ë³„ ëŒ€í‘œ íƒœê·¸ ìš°ì„  ì„ ë³„
        for t, c in valid_candidates:
            matched = None
            for root, keywords in clusters.items():
                if any(k in t for k in keywords):
                    matched = root; break
            if matched and matched not in used_roots:
                final_tags.append((t, c)); used_roots.add(matched)

        # C-2. ë‚˜ë¨¸ì§€ ë¹ˆìë¦¬ ì±„ìš°ê¸° (í¬í•¨ ê´€ê³„ ì¤‘ë³µ ì œê±°)
        for t, c in valid_candidates:
            if len(final_tags) >= 10: break
            if any(t == existing[0] for existing in final_tags): continue
            
            is_redundant = False
            for ex_t, _ in final_tags:
                if t in ex_t or ex_t in t: # í¬í•¨ ê´€ê³„ ì²´í¬
                    is_redundant = True; break
            if not is_redundant: final_tags.append((t, c))
            
        final_recommendation = sorted(final_tags, key=lambda x: x[1], reverse=True)[:10]

        return fixed_keywords, readable_auto_pairs, spec_counts, final_recommendation, raw_tag_stats

    def create_excel_download(self, fixed_keys, auto_keys, specs, tags, raw_stats):
        """ê²°ê³¼ë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ë³€í™˜"""
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            # ìš”ì•½ ì‹œíŠ¸
            summary_data = {
                'êµ¬ë¶„': ['ì™„ì„±ëœ ìƒí’ˆëª…', 'ì¶”ì²œ íƒœê·¸(10ì„ )'],
                'ë‚´ìš©': [
                    " ".join(fixed_keys + [p[0] for p in auto_keys]),
                    ", ".join([f"#{t[0]}" for t in tags])
                ]
            }
            pd.DataFrame(summary_data).to_excel(writer, sheet_name='ìµœì í™”_ìš”ì•½', index=False)
            
            # ìƒì„¸ ë°ì´í„° ì‹œíŠ¸
            pd.DataFrame(auto_keys, columns=['í‚¤ì›Œë“œ', 'ë¹ˆë„']).to_excel(writer, sheet_name='ìƒí’ˆëª…_í‚¤ì›Œë“œ', index=False)
            pd.DataFrame(specs, columns=['ì†ì„±', 'ë¹ˆë„']).to_excel(writer, sheet_name='ì†ì„±_ë¶„ì„', index=False)
            pd.DataFrame(raw_stats, columns=['íƒœê·¸ëª…', 'ì‹¤ì œì‚¬ìš©ë¹ˆë„']).to_excel(writer, sheet_name='ì „ì²´_íƒœê·¸_í†µê³„', index=False)
            
        return output.getvalue()

# 3. GUI êµ¬ì„±
st.sidebar.header("ğŸ“ Step 1. ë°ì´í„° ì—…ë¡œë“œ")
uploaded_file = st.sidebar.file_uploader("ë¶„ì„ìš© CSV íŒŒì¼ ì—…ë¡œë“œ", type=["csv"])

st.sidebar.header("ğŸ¯ Step 2. ì „ëµ í‚¤ì›Œë“œ ì„¤ì •")
conversion_input = st.sidebar.text_input("êµ¬ë§¤ì „í™˜ í‚¤ì›Œë“œ", placeholder="ì˜ˆ: ë§›ìˆëŠ” ì†í¸í•œ")
add_input = st.sidebar.text_input("ì¶”ê°€í•  í‚¤ì›Œë“œ (ê³ ì • ë°°ì¹˜)", placeholder="ì˜ˆ: êµ­ë‚´ì‚° ë‹¹ì¼ë°œì†¡")
exclude_input = st.sidebar.text_input("ì œì™¸í•  í‚¤ì›Œë“œ (ë¶„ì„ ì œì™¸)", placeholder="ì˜ˆ: ë¸Œëœë“œëª…")
total_kw_count = st.sidebar.number_input("ìƒí’ˆëª… ì´ í‚¤ì›Œë“œ ìˆ˜", min_value=5, max_value=25, value=11)

user_exclude_list = [w.strip() for w in exclude_input.split() if len(w.strip()) > 0]

if uploaded_file:
    try:
        df = pd.read_csv(uploaded_file, encoding='cp949')
    except:
        uploaded_file.seek(0)
        df = pd.read_csv(uploaded_file, encoding='utf-8-sig')

    manager = SEOManager(df, user_exclude_list)
    fixed, auto, specs, rec_tags, raw_stats = manager.run_analysis(conversion_input, add_input, total_kw_count)

    st.success(f"âœ¨ ë¶„ì„ ì™„ë£Œ! (ì´ {total_kw_count}ê°œ í‚¤ì›Œë“œ ì¡°í•©)")

    # ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
    excel_data = manager.create_excel_download(fixed, auto, specs, rec_tags, raw_stats)
    st.download_button(
        label="ğŸ“¥ ë¶„ì„ ê²°ê³¼ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
        data=excel_data,
        file_name="SEO_ë¶„ì„ê²°ê³¼.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )

    # ì„¹ì…˜ 1: ìƒí’ˆëª…
    st.header("ğŸ·ï¸ 1. ì „ëµì  ìƒí’ˆëª… ì¡°í•©")
    col1, col2 = st.columns([2, 1])
    with col1:
        st.subheader("âœ… ì™„ì„±ëœ ìƒí’ˆëª…")
        full_title = " ".join(fixed + [p[0] for p in auto])
        st.code(full_title, language=None)
        st.info("**ê°€ë…ì„± ì „ëµ:** [êµ¬ë§¤ì „í™˜] + [ì¶”ê°€] + [ë³¸ì§ˆ] + [ì œí˜•] + [ìš©ë„] + [ì†ì„±]")
    with col2:
        st.subheader("ğŸ“Š í‚¤ì›Œë“œ ë¹ˆë„")
        df_auto = pd.DataFrame(auto, columns=['ë‹¨ì–´', 'ë¹ˆë„'])
        df_auto.index += 1
        st.table(df_auto)

    st.markdown("---")

    # ì„¹ì…˜ 2: ì†ì„±
    st.header("âš™ï¸ 2. í•„í„° ë…¸ì¶œìš© ì†ì„±ê°’")
    col3, col4 = st.columns([2, 1])
    with col3:
        for s, _ in specs: st.button(s, key=s, use_container_width=True)
    with col4:
        df_spec = pd.DataFrame(specs, columns=['ì†ì„±ê°’', 'ë¹ˆë„'])
        df_spec.index += 1
        st.table(df_spec)

    st.markdown("---")

    # ì„¹ì…˜ 3: íƒœê·¸ (ì •ë°€ì„± + í™•ì¥ì„±)
    st.header("ğŸ” 3. í™•ì¥ ê²€ìƒ‰ íƒœê·¸")
    col5, col6 = st.columns([2, 1])
    with col5:
        st.subheader("âœ… ìµœì í™” íƒœê·¸ 10ì„  (ì¶”ì²œ)")
        st.warning(", ".join([f"#{t[0]}" for t in rec_tags]))
        st.info("**ìµœì í™”:** ì¤‘ë³µ ì˜ë¯¸ ë°°ì œ ë° ì¹´í…Œê³ ë¦¬ í™•ì¥ ì ìš©")
    with col6:
        st.subheader("ğŸ“Š ì›ë³¸ ì‚¬ìš© ë¹ˆë„ (Top 20)")
        df_raw = pd.DataFrame(raw_stats[:20], columns=['íƒœê·¸ëª…', 'ì‹¤ì œ ë¹ˆë„'])
        df_raw.index += 1
        st.table(df_raw)

else:
    st.info("íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ë¶„ì„ì´ ì‹œì‘ë©ë‹ˆë‹¤.")
