import streamlit as st
import pandas as pd
import re
from collections import Counter

# 1. í˜ì´ì§€ ì„¤ì • ë° ë””ìì¸
st.set_page_config(page_title="ë„¤ì´ë²„ SEO NLU ë§ˆìŠ¤í„°", layout="wide")
st.title("ğŸš€ ë„¤ì´ë²„ ì‡¼í•‘ SEO í†µí•© ìµœì í™” ë§¤ë‹ˆì €")
st.markdown("---")

# 2. ì „ë¬¸ SEO ë¶„ì„ ë¡œì§ í´ë˜ìŠ¤
class SEOManager:
    def __init__(self, df, user_exclude_list):
        self.df = df
        self.exclude_brands = set([
            'ë§¤ì¼', 'ì„œìš¸ìš°ìœ ', 'ì„œìš¸', 'ì—°ì„¸', 'ë‚¨ì–‘', 'ê±´êµ­', 'íŒŒìŠ¤í‡´ë¥´', 'ì¼ë™', 'í›„ë””ìŠ¤', 
            'ì†Œì™€ë‚˜ë¬´', 'ë¹™ê·¸ë ˆ', 'ì…€ë¡œëª¬', 'ë¹…ì›ë”', 'ë¯¸ê´‘ìŠ¤í† ì–´', 'ë°ì–´ë¦¬ë§ˆì¼“', 'ë„ë‚¨ìƒíšŒ', 
            'í¬ì°½ìœ ì—…', 'ë‹´í„°', 'ì—°ì„¸ìœ ì—…', 'ë§¤ì¼ìœ ì—…'
        ] + user_exclude_list)
        # NLU ë¶„ë¦¬ ê¸°ì¤€ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ (ê¸´ ë‹¨ì–´ë¶€í„° ë§¤ì¹­í•˜ì—¬ ì˜¤ì°¨ ë°©ì§€)
        self.sub_splits = sorted([
            'ìíŒê¸°', 'ìš°ìœ ', 'ë¶„ìœ ', 'ê°€ë£¨', 'ë¶„ë§', 'ì „ì§€', 'íƒˆì§€', 'ìŠ¤í‹±', 
            'ì—…ì†Œìš©', 'ëŒ€ìš©ëŸ‰', 'ì‹ìì¬', 'ì œê³¼', 'ì œë¹µ', 'ë² ì´í‚¹', 'ë©¸ê· ', 'íŒŒìš°ì¹˜', 'ì „ì§€ë°€'
        ], key=len, reverse=True)

    def split_base_terms(self, text):
        """ë³µí•© ëª…ì‚¬ë¥¼ NLU ê·œì¹™ì— ë”°ë¼ ì¡°ê° í‚¤ì›Œë“œë¡œ ë¶„ë¦¬í•˜ëŠ” í•µì‹¬ ì—”ì§„"""
        if pd.isna(text) or text == '-': return []
        
        # 1. íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ê³µë°± ì •ê·œí™”
        text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', ' ', str(text))
        
        # 2. ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ ìƒì„± (ì´ë¯¸ ì •ì˜ëœ í•µì‹¬ í‚¤ì›Œë“œ ê¸°ì¤€)
        pattern = f"({'|'.join(self.sub_splits)})"
        
        # 3. í…ìŠ¤íŠ¸ ë¶„ë¦¬ ì‹¤í–‰
        raw_parts = re.split(pattern, text)
        
        terms = []
        for part in raw_parts:
            p = part.strip()
            if not p: continue
            
            # ë¸Œëœë“œëª…ì´ë‚˜ ìˆ«ìê°€ í¬í•¨ëœ ì¡°ê°ì€ ì œì™¸
            if p in self.exclude_brands or any(c.isdigit() for c in p):
                continue
                
            # ë‹¨ì–´ ê¸¸ì´ê°€ 2ì ì´ìƒì´ê±°ë‚˜, NLU í•µì‹¬ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ëœ ê²½ìš° ìˆ˜ì§‘
            if len(p) > 1 or p in self.sub_splits:
                # ë‚´ë¶€ ê³µë°±ì´ ìˆì„ ê²½ìš° ë‹¤ì‹œ ìª¼ê°œì–´ ì¶”ê°€
                terms.extend(p.split())
                
        return terms

    def reorder_for_readability(self, word_count_pairs):
        """ê°€ë…ì„± ê·¸ë£¹ë³„ ì¬ë°°ì¹˜ (ë³¸ì§ˆ->ì œí˜•->ìš©ë„->ì†ì„±)"""
        identity = ['ì „ì§€', 'ë¶„ìœ ', 'ìš°ìœ ', 'íƒˆì§€', 'ì „ì§€ë°€']
        form = ['ë¶„ë§', 'ê°€ë£¨', 'ìŠ¤í‹±', 'ì•¡ìƒ']
        usage = ['ìíŒê¸°', 'ì—…ì†Œìš©', 'ëŒ€ìš©ëŸ‰', 'ì‹ìì¬', 'ì œê³¼', 'ì œë¹µ', 'ë² ì´í‚¹']
        desc = ['ì§„í•œ', 'ê³ ì†Œí•œ', 'ë§›ìˆëŠ”', 'ì¶”ì–µ', 'ì¶”ì²œ', 'ì†í¸í•œ']

        def get_priority(pair):
            word = pair[0]
            if any(core in word for core in identity): return 1
            if any(core in word for core in form): return 2
            if any(core in word for core in usage): return 3
            if any(core in word for core in desc): return 4
            return 5
        return sorted(word_count_pairs, key=lambda x: get_priority(x))

    def run_analysis(self, conversion_input, add_input, total_target_count):
        # --- ìˆ˜ì • í¬ì¸íŠ¸: ìˆ˜ë™ ì…ë ¥ê°’ì—ë„ NLU ë¶„ë¦¬ ì—”ì§„ ì ìš© ---
        conv_keys = self.split_base_terms(conversion_input)
        add_keys = self.split_base_terms(add_input)
        fixed_keywords = conv_keys + add_keys
        
        # ìƒí’ˆëª… ê¸°ë°˜ ë¹ˆë„ ë¶„ì„
        name_terms = []
        for name in self.df['ìƒí’ˆëª…']:
            name_terms.extend(self.split_base_terms(name))
        
        name_freq = Counter(name_terms).most_common(50)
        auto_candidates = []
        for w, c in name_freq:
            # ì™„ì „ ì¼ì¹˜ ë¹„êµë¡œ ì¤‘ë³µ ì œê±°
            if w not in fixed_keywords:
                auto_candidates.append((w, c))
        
        remain_count = max(0, total_target_count - len(fixed_keywords))
        selected_auto_pairs = auto_candidates[:remain_count]
        readable_auto_pairs = self.reorder_for_readability(selected_auto_pairs)
        
        # ìŠ¤í™ ë° íƒœê·¸ ë¶„ì„ ë¡œì§
        spec_list = []
        for spec in self.df['ìŠ¤í™'].dropna():
            parts = [p.strip() for p in str(spec).split('|') if len(p.strip()) > 1]
            spec_list.extend([p for p in parts if p not in self.exclude_brands])
        spec_counts = Counter(spec_list).most_common(8)

        tag_raw_list = []
        for row in self.df['ê²€ìƒ‰ì¸ì‹íƒœê·¸'].dropna():
            tag_raw_list.extend([t.strip() for t in str(row).split(',') if t.strip()])
        
        tag_freq = Counter(tag_raw_list).most_common(150)
        current_title_set = set(fixed_keywords + [p[0] for p in readable_auto_pairs])
        
        # íƒœê·¸ í›„ë³´ ì„ ë³„ (ì¤‘ë³µ ì œê±° ë° ìˆ˜ì‹ì–´ ë‹¤ì–‘í™”)
        candidates = [(t, c) for t, c in tag_freq if not any(c.isdigit() for c in t) and t not in current_title_set]
        final_tags = []
        top_candidates = candidates[:40]
        
        for i, (target_t, target_c) in enumerate(top_candidates):
            if len(final_tags) >= 10: break
            # í¬í•¨ ê´€ê³„ ë° ìˆ˜ì‹ì–´ ì¤‘ë³µ ë°°ì œ (í™•ì¥ì„± ì „ëµ)
            prefix = target_t[:3] if len(target_t) > 3 else target_t[:2]
            is_redundant = False
            for ex_t, _ in final_tags:
                if prefix in ex_t or any(ex_t[:3] in target_t for ex_t, _ in final_tags if len(ex_t) > 2):
                    is_redundant = True; break
                if target_t in ex_t: is_redundant = True; break
            
            if not is_redundant:
                final_tags.append((target_t, target_c))

        return fixed_keywords, readable_auto_pairs, spec_counts, sorted(final_tags, key=lambda x: x[1], reverse=True)[:10]

def calculate_seo_metrics(text):
    char_count = len(text)
    try: byte_count = len(text.encode('euc-kr'))
    except: byte_count = len(text.encode('utf-8'))
    return char_count, byte_count

# 3. GUI êµ¬ì„±
st.sidebar.header("ğŸ¯ ì „ëµ í‚¤ì›Œë“œ ì„¤ì •")
uploaded_file = st.sidebar.file_uploader("ë¶„ì„ìš© CSV ì—…ë¡œë“œ", type=["csv"])
conversion_input = st.sidebar.text_input("êµ¬ë§¤ì „í™˜ í‚¤ì›Œë“œ", placeholder="ì˜ˆ: ë§›ìˆëŠ”ìíŒê¸°ìš°ìœ ")
add_input = st.sidebar.text_input("ì¶”ê°€í•  í‚¤ì›Œë“œ", placeholder="ì˜ˆ: ë¬´ë£Œë°°ì†¡ë‹¹ì¼ë°œì†¡")
total_kw_count = st.sidebar.number_input("ëª©í‘œ í‚¤ì›Œë“œ ìˆ˜", min_value=5, max_value=25, value=11)

if uploaded_file:
    try:
        df = pd.read_csv(uploaded_file, encoding='cp949')
    except:
        uploaded_file.seek(0)
        df = pd.read_csv(uploaded_file, encoding='utf-8-sig')

    manager = SEOManager(df, [])
    fixed, auto, specs, tags = manager.run_analysis(conversion_input, add_input, total_kw_count)

    st.header("ğŸ·ï¸ 1. ì „ëµì  ìƒí’ˆëª… ì¡°í•©")
    col1, col2 = st.columns([2, 1])
    with col1:
        st.subheader("âœ… ì™„ì„±ëœ ìƒí’ˆëª…")
        full_title = " ".join(fixed + [p[0] for p in auto])
        st.code(full_title, language=None)
        
        c_len, b_len = calculate_seo_metrics(full_title)
        total_used_kw = len(fixed) + len(auto)
        
        status = "ğŸŸ¢ ì •ìƒ" if c_len <= 50 else "ğŸ”´ ì£¼ì˜"
        st.markdown(f"**{status}**: {c_len}ì / {b_len} Byte / {total_used_kw}ê°œ í‚¤ì›Œë“œ")
        st.info("**NLU ê·œì¹™ ì ìš©:** ì…ë ¥í•˜ì‹  í‚¤ì›Œë“œê°€ ìë™ìœ¼ë¡œ ë¶„ë¦¬ ë° ì •ë ¬ë˜ì—ˆìŠµë‹ˆë‹¤.")

    with col2:
        st.subheader("ğŸ“Š ìë™ ì¶”ì²œ ë¹ˆë„")
        st.table(pd.DataFrame(auto, columns=['ë‹¨ì–´', 'ë¹ˆë„']).assign(No=range(1, len(auto)+1)).set_index('No'))

    st.markdown("---")
    
    # ì´ë¯¸ì§€ 77c3cc ë ˆì´ì•„ì›ƒ ì ìš©
    st.header("âš™ï¸ 2. í•„í„° ì†ì„± & ğŸ” 3. í™•ì¥ íƒœê·¸")
    l_col, r_col = st.columns(2)
    with l_col:
        for s_name, _ in specs:
            st.button(s_name, use_container_width=True, key=f"attr_{s_name}")
    with r_col:
        tag_str = ", ".join([f"#{t[0]}" for t in tags])
        st.success(tag_str)
        st.caption("â€» ì¤‘ë³µ ìˆ˜ì‹ì–´ë¥¼ ë°°ì œí•˜ê³  ìœ ì… ê²½ë¡œë¥¼ ê·¹ëŒ€í™”í•œ íƒœê·¸ì…ë‹ˆë‹¤.")
else:
    st.info("íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ì •ë°€ SEO ë¶„ì„ì´ ì‹œì‘ë©ë‹ˆë‹¤.")
