import streamlit as st
import pandas as pd
import re
from collections import Counter

# 1. í˜ì´ì§€ ì œëª© ë° ë ˆì´ì•„ì›ƒ
st.set_page_config(page_title="ì‚¬ë‚´ SEO ë¶„ì„ ë„êµ¬", layout="wide")
st.title("ğŸ“Š ì „ì§ì› ê³µìš© ë„¤ì´ë²„ SEO ìµœì í™” ë„êµ¬")
st.markdown("---")

# 2. ì œì™¸ ë‹¨ì–´ ì„¤ì • (ë¸Œëœë“œëª… ë“±)
EXCLUDE_WORDS = ['ë§¤ì¼', 'ì„œìš¸ìš°ìœ ', 'ì„œìš¸', 'ì—°ì„¸', 'ë‚¨ì–‘', 'ê±´êµ­', 'íŒŒìŠ¤í‡´ë¥´', 'ì¼ë™', 'í›„ë””ìŠ¤', 'ì…€ë¡œëª¬', 'í¬ì°½ìœ ì—…']

# 3. í•µì‹¬ ë¶„ì„ í•¨ìˆ˜
def analyze_seo(df):
    # NLU ê¸°ë°˜ ë‹¨ì–´ ìª¼ê°œê¸°
    all_names = []
    for name in df['ìƒí’ˆëª…']:
        clean_n = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', ' ', str(name))
        words = [w for w in clean_n.split() if len(w) > 1 and w not in EXCLUDE_WORDS and not w.isdigit()]
        all_names.extend(words)
    
    # ìƒìœ„ 12ê°œ í…€ ì¶”ì¶œ (nluterms ê¸°ì¤€)
    top_names = [w for w, c in Counter(all_names).most_common(12)]
    
    # íƒœê·¸ ì¶”ì¶œ (ì¤‘ë³µ ì œê±° ë¡œì§)
    all_tags = []
    for t_row in df['ê²€ìƒ‰ì¸ì‹íƒœê·¸'].dropna():
        if t_row != '-':
            ts = [t.strip() for t in str(t_row).split(',') if t.strip() not in EXCLUDE_WORDS]
            all_tags.extend(ts)
    
    final_tags = []
    for t in [w for w, c in Counter(all_tags).most_common(50)]:
        if len(final_tags) >= 10: break
        if not any(word in t for word in top_names):
            final_tags.append(t)
            
    return top_names, final_tags

# 4. íŒŒì¼ ì—…ë¡œë“œ ë° ê²°ê³¼ í‘œì‹œ
uploaded_file = st.file_uploader("ë¶„ì„í•  CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["csv"])

if uploaded_file:
    try:
        df = pd.read_csv(uploaded_file, encoding='cp949')
    except:
        df = pd.read_csv(uploaded_file, encoding='utf-8-sig')
    
    names, tags = analyze_seo(df)
    
    st.subheader("âœ… ì¶”ì²œ ìƒí’ˆëª… (11~12ë‹¨ì¶• ì¡°í•©)")
    st.code(" ".join(names), language=None)
    
    st.subheader("âœ… ì¶”ì²œ íƒœê·¸ (ì¤‘ë³µ ë°°ì œ 10ì„ )")
    st.info(", ".join([f"#{t}" for t in tags]))
