import streamlit as st
import pandas as pd
import re
from collections import Counter

# 1. í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ë„¤ì´ë²„ SEO NLU ë§ˆìŠ¤í„°", layout="wide")
st.title("ğŸš€ ë„¤ì´ë²„ ì‡¼í•‘ SEO í†µí•© ìµœì í™” (ê²€ìƒ‰ í™•ì¥ì„± ë§ˆìŠ¤í„°)")
st.markdown("---")

class SEOManager:
    def __init__(self, df, user_exclude_list):
        self.df = df
        self.exclude_brands = [
            'ë§¤ì¼', 'ì„œìš¸ìš°ìœ ', 'ì„œìš¸', 'ì—°ì„¸', 'ë‚¨ì–‘', 'ê±´êµ­', 'íŒŒìŠ¤í‡´ë¥´', 'ì¼ë™', 'í›„ë””ìŠ¤', 
            'ì†Œì™€ë‚˜ë¬´', 'ë¹™ê·¸ë ˆ', 'ì…€ë¡œëª¬', 'ë¹…ì›ë”', 'ë¯¸ê´‘ìŠ¤í† ì–´', 'ë°ì–´ë¦¬ë§ˆì¼“', 'ë„ë‚¨ìƒíšŒ', 
            'í¬ì°½ìœ ì—…', 'ë‹´í„°', 'ì—°ì„¸ìœ ì—…', 'ë§¤ì¼ìœ ì—…'
        ] + user_exclude_list

    def split_base_terms(self, text):
        if pd.isna(text) or text == '-': return []
        text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', ' ', str(text))
        raw_words = text.split()
        terms = []
        sub_splits = ['ìíŒê¸°', 'ìš°ìœ ', 'ë¶„ìœ ', 'ê°€ë£¨', 'ë¶„ë§', 'ì „ì§€', 'íƒˆì§€', 'ìŠ¤í‹±', 'ì—…ì†Œìš©', 'ëŒ€ìš©ëŸ‰']
        for word in raw_words:
            if word in self.exclude_brands or any(char.isdigit() for char in word): continue
            found_sub = False
            for sub in sub_splits:
                if sub in word and word != sub:
                    terms.append(sub)
                    rem = word.replace(sub, '').strip()
                    if len(rem) > 1 and not any(char.isdigit() for char in rem) and rem not in self.exclude_brands:
                        terms.append(rem)
                    found_sub = True
                    break
            if not found_sub and len(word) > 1: terms.append(word)
        return terms

    def reorder_for_readability(self, word_count_pairs):
        identity, form, usage, desc = ['ì „ì§€', 'ë¶„ìœ ', 'ìš°ìœ ', 'íƒˆì§€'], ['ë¶„ë§', 'ê°€ë£¨', 'ìŠ¤í‹±', 'ì•¡ìƒ'], ['ìíŒê¸°', 'ì—…ì†Œìš©', 'ëŒ€ìš©ëŸ‰', 'ì‹ìì¬', 'ì œê³¼', 'ì œë¹µ'], ['ì§„í•œ', 'ê³ ì†Œí•œ', 'ë§›ìˆëŠ”', 'ì¶”ì–µ']
        def get_priority(pair):
            word = pair[0]
            if any(core in word for core in identity): return 1
            if any(core in word for core in form): return 2
            if any(core in word for core in usage): return 3
            if any(core in word for core in desc): return 4
            return 5
        return sorted(word_count_pairs, key=lambda x: get_priority(x))

    def run_analysis(self, conversion_input, add_input, total_target_count):
        conv_keys = [w.strip() for w in conversion_input.split() if w.strip()]
        add_keys = [w.strip() for w in add_input.split() if w.strip()]
        fixed_keywords = conv_keys + add_keys
        
        name_terms = []
        for name in self.df['ìƒí’ˆëª…']: name_terms.extend(self.split_base_terms(name))
        name_freq = Counter(name_terms).most_common(50)
        auto_candidates = [(w, c) for w, c in name_freq if not any(fixed_w in w or w in fixed_w for fixed_w in fixed_keywords)]
        
        auto_pairs = auto_candidates[:max(0, total_target_count - len(fixed_keywords))]
        readable_auto = self.reorder_for_readability(auto_pairs)
        
        # íƒœê·¸ ë¶„ì„ ë¡œì§ - â˜… ìˆ˜ì‹ì–´ ì¤‘ë³µ ë°°ì œ ë° í™•ì¥ì„± ê°•í™” â˜…
        tag_raw_list = []
        for tags in self.df['ê²€ìƒ‰ì¸ì‹íƒœê·¸'].dropna():
            if tags != '-':
                tag_raw_list.extend([t.strip() for t in str(tags).split(',') if not any(b in t for b in self.exclude_brands)])
        
        tag_freq = Counter(tag_raw_list).most_common(150)
        title_words = set(fixed_keywords + [p[0] for p in readable_auto])
        
        candidates = [(t, c) for t, c in tag_freq if not any(char.isdigit() for char in t) and t not in title_words]

        final_tags = []
        used_prefixes = set() # "ì¶”ì–µì˜", "ë§›ìˆëŠ”" ë“± ì•ë¶€ë¶„ ì¤‘ë³µ ì²´í¬ìš©

        for t, c in candidates:
            if len(final_tags) >= 10: break
            
            # ìˆ˜ì‹ì–´ ì¶”ì¶œ (ì• 3ê¸€ì ê¸°ì¤€ ë˜ëŠ” íŠ¹ì • íŒ¨í„´)
            # ì˜ˆ: "ì¶”ì–µì˜ë§›" -> "ì¶”ì–µì˜", "ì•„ì´ê°„ì‹" -> "ì•„ì´"
            prefix = t[:3] if len(t) > 3 else t[:2]
            
            # ì´ë¯¸ ì„ ì ëœ ìˆ˜ì‹ì–´ì´ê±°ë‚˜ ìƒí˜¸ í¬í•¨ ê´€ê³„ì¸ ê²½ìš° ê±´ë„ˆëœ€ (í™•ì¥ì„± ê·¹ëŒ€í™”)
            is_redundant = False
            for existing_t, _ in final_tags:
                # 1. ìˆ˜ì‹ì–´(ì•ë¶€ë¶„)ê°€ ê²¹ì¹˜ëŠ”ì§€ ì²´í¬ (ì¶”ì–µì˜ë§› vs ì¶”ì–µì˜ê°„ì‹ ë°©ì§€)
                if prefix in existing_t or any(existing_t[:3] in t for existing_t, _ in final_tags if len(existing_t) > 2):
                    is_redundant = True
                # 2. í¬í•¨ ê´€ê³„ ì²´í¬ (ì œê³¼ì œë¹µ vs ì œê³¼ì œë¹µì¬ë£Œ ì¤‘ ê¸´ ê²ƒ ì„ íƒ)
                if t in existing_t: # ë” ê¸´ ë‹¨ì–´ê°€ ì´ë¯¸ ìˆìŒ
                    is_redundant = True
                elif existing_t in t: # í˜„ì¬ ë‹¨ì–´ê°€ ë” ê¸¸ë©´ êµì²´ ë¡œì§ (ì´ë²ˆ íšŒì°¨ì—ì„  ìƒëµí•˜ê³  ë‹¤ìŒìœ¼ë¡œ ë„˜ê¹€)
                    is_redundant = True
            
            if not is_redundant:
                final_tags.append((t, c))
                used_prefixes.add(prefix)

        # 10ê°œê°€ ì•ˆ ì±„ì›Œì¡Œì„ ê²½ìš° ë³´ì¶©
        for t, c in candidates:
            if len(final_tags) >= 10: break
            if not any(t == ex[0] for ex in final_tags):
                final_tags.append((t, c))

        return conv_keys, add_keys, readable_auto, sorted(final_tags, key=lambda x: x[1], reverse=True)[:10]

def check_seo(text):
    c_len = len(text)
    try: b_len = len(text.encode('euc-kr'))
    except: b_len = len(text.encode('utf-8'))
    return c_len, b_len

# 3. UI
st.sidebar.header("ğŸ“ Step 1. ë°ì´í„° ì—…ë¡œë“œ")
uploaded_file = st.sidebar.file_uploader("ë¶„ì„ìš© CSV íŒŒì¼ ì—…ë¡œë“œ", type=["csv"])

st.sidebar.header("ğŸ¯ Step 2. ì „ëµ í‚¤ì›Œë“œ ì„¤ì •")
conv_in = st.sidebar.text_input("êµ¬ë§¤ì „í™˜ í‚¤ì›Œë“œ")
add_in = st.sidebar.text_input("ì¶”ê°€í•  í‚¤ì›Œë“œ")
total_kw = st.sidebar.number_input("ìƒí’ˆëª… ì´ í‚¤ì›Œë“œ ìˆ˜", min_value=5, max_value=25, value=11)

if uploaded_file:
    df = pd.read_csv(uploaded_file, encoding='cp949') if 'cp949' else pd.read_csv(uploaded_file, encoding='utf-8-sig')
    manager = SEOManager(df, [])
    conv, add, auto, tags = manager.run_analysis(conv_in, add_in, total_kw)

    st.header("ğŸ·ï¸ 1. ì „ëµì  ìƒí’ˆëª… ì¡°í•©")
    full_title = " ".join(conv + add + [p[0] for p in auto])
    c_len, b_len = check_seo(full_title)
    
    col1, col2 = st.columns([2, 1])
    with col1:
        st.code(full_title, language=None)
        st.markdown(f"**ìƒíƒœ:** {'ğŸŸ¢ ì •ìƒ' if c_len <= 50 else 'ğŸ”´ ì´ˆê³¼'} | **ê¸€ììˆ˜:** {c_len}ì | **ë°”ì´íŠ¸:** {b_len}B")
        st.metric("ì´ ì‚¬ìš© í‚¤ì›Œë“œ ìˆ˜", f"{len(conv)+len(add)+len(auto)}ê°œ")
    with col2:
        st.table(pd.DataFrame(auto, columns=['ë‹¨ì–´', 'ë¹ˆë„']).assign(No=range(1, len(auto)+1)).set_index('No'))

    st.markdown("---")
    st.header("ğŸ” 3. í™•ì¥ ê²€ìƒ‰ íƒœê·¸ (ì¡°í•© í™•ì¥ì„± ê·¹ëŒ€í™”)")
    t_col1, t_col2 = st.columns([2, 1])
    with t_col1:
        st.success(", ".join([f"#{t[0]}" for t in tags]))
        st.info("ğŸ’¡ **í™•ì¥ ì „ëµ ì ìš©:** 'ì¶”ì–µì˜'ì™€ ê°™ì€ ì¤‘ë³µ ìˆ˜ì‹ì–´ë¥¼ ë°°ì œí•˜ê³ , ìµœëŒ€í•œ ë‹¤ì–‘í•œ ìœ ì… ê²½ë¡œ(ë§›, ìš©ë„, íƒ€ê²Ÿ)ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤.")
    with t_col2:
        st.table(pd.DataFrame(tags, columns=['íƒœê·¸ëª…', 'ì‚¬ìš© ë¹ˆë„ìˆ˜']).assign(No=range(1, len(tags)+1)).set_index('No'))
