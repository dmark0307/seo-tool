import streamlit as st
import pandas as pd
import re
from collections import Counter

# 1. í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ë„¤ì´ë²„ SEO í†µí•© ë¶„ì„ ë„êµ¬", layout="wide")
st.title("ğŸš€ ë„¤ì´ë²„ ì‡¼í•‘ SEO í†µí•© ìµœì í™” ë§¤ë‹ˆì €")
st.markdown("---")

# 2. ì „ë¬¸ SEO ë¶„ì„ ë¡œì§ í´ë˜ìŠ¤
class SEOManager:
    def __init__(self, df):
        self.df = df
        self.exclude_brands = [
            'ë§¤ì¼', 'ì„œìš¸ìš°ìœ ', 'ì„œìš¸', 'ì—°ì„¸', 'ë‚¨ì–‘', 'ê±´êµ­', 'íŒŒìŠ¤í‡´ë¥´', 'ì¼ë™', 'í›„ë””ìŠ¤', 
            'ì†Œì™€ë‚˜ë¬´', 'ë¹™ê·¸ë ˆ', 'ì…€ë¡œëª¬', 'ë¹…ì›ë”', 'ë¯¸ê´‘ìŠ¤í† ì–´', 'ë°ì–´ë¦¬ë§ˆì¼“', 'ë„ë‚¨ìƒíšŒ', 
            'í¬ì°½ìœ ì—…', 'ë‹´í„°', 'ì—°ì„¸ìœ ì—…', 'ë§¤ì¼ìœ ì—…'
        ]

    def split_base_terms(self, text):
        """ë³µí•© ëª…ì‚¬ë¥¼ ë¶„ë¦¬í•˜ì—¬ ê¸°ì´ˆ ë‹¨ì–´(Base Term) ì¶”ì¶œ"""
        if pd.isna(text) or text == '-': return []
        text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', ' ', str(text))
        raw_words = text.split()
        
        terms = []
        sub_splits = ['ìíŒê¸°', 'ìš°ìœ ', 'ë¶„ìœ ', 'ê°€ë£¨', 'ë¶„ë§', 'ì „ì§€', 'íƒˆì§€', 'ìŠ¤í‹±', 'ì—…ì†Œìš©', 'ëŒ€ìš©ëŸ‰']
        
        for word in raw_words:
            if word in self.exclude_brands or word.isdigit(): continue
            found_sub = False
            for sub in sub_splits:
                if sub in word and word != sub:
                    terms.append(sub)
                    rem = word.replace(sub, '').strip()
                    if len(rem) > 1: terms.append(rem)
                    found_sub = True
                    break
            if not found_sub and len(word) > 1:
                terms.append(word)
        return terms

    def run_analysis(self):
        # [ìƒí’ˆëª… ë¶„ì„]
        name_terms = []
        for name in self.df['ìƒí’ˆëª…']:
            name_terms.extend(self.split_base_terms(name))
        name_counts = Counter(name_terms).most_common(20)
        top_12_names = [w for w, c in name_counts[:12]]

        # [ì†ì„± ë¶„ì„]
        spec_list = []
        for spec in self.df['ìŠ¤í™'].dropna():
            if spec != '-':
                parts = [p.strip() for p in str(spec).split('|')]
                spec_list.extend([p for p in parts if len(p) > 1 and p not in self.exclude_brands])
        spec_counts = Counter(spec_list).most_common(10)

        # [íƒœê·¸ ë¶„ì„ - í‚¤ì›Œë“œ í™•ì¥ì„± ê·¹ëŒ€í™” ë¡œì§]
        tag_list = []
        for tags in self.df['ê²€ìƒ‰ì¸ì‹íƒœê·¸'].dropna():
            if tags != '-':
                parts = [t.strip() for t in str(tags).split(',')]
                tag_list.extend([t for t in parts if not any(b in t for b in self.exclude_brands)])
        
        tag_freq = Counter(tag_list).most_common(100)
        
        # 1ë‹¨ê³„: ìƒí’ˆëª… ì¤‘ë³µ ë‹¨ì–´ ì œê±°
        candidates = []
        for t, c in tag_freq:
            if not any(word in t for word in top_12_names):
                candidates.append({'tag': t, 'count': c})
        
        # 2ë‹¨ê³„: í‚¤ì›Œë“œ í™•ì¥ ë¡œì§ (Aê°€ Bì— í¬í•¨ë˜ë©´ Aë¥¼ ë²„ë¦¬ê³  Bë¥¼ ìœ ì§€)
        # ì˜ˆ: 'ì œê³¼ì œë¹µ'ì´ 'ì œê³¼ì œë¹µì¬ë£Œ'ì— í¬í•¨ë˜ë©´ 'ì œê³¼ì œë¹µ'ì€ íƒˆë½ì‹œí‚´
        tags_to_skip = set()
        for i in range(len(candidates)):
            t1 = candidates[i]['tag']
            for j in range(len(candidates)):
                if i == j: continue
                t2 = candidates[j]['tag']
                # ë” ê¸´ ë‹¨ì–´ê°€ ì§§ì€ ë‹¨ì–´ë¥¼ í¬í•¨í•˜ê³  ìˆë‹¤ë©´ ì§§ì€ ë‹¨ì–´(t1)ë¥¼ ìŠ¤í‚µ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                if t1 in t2:
                    tags_to_skip.add(t1)
                    break
        
        final_pool = [c for c in candidates if c['tag'] not in tags_to_skip]
        
        # 3ë‹¨ê³„: ìµœì¢… ë¹ˆë„ìˆ˜ ìˆœìœ¼ë¡œ 10ê°œ ì„ ë³„
        final_tags_with_count = [(c['tag'], c['count']) for c in final_pool[:10]]
        
        return name_counts[:12], spec_counts[:8], final_tags_with_count

# 3. ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ ë° ê²°ê³¼ ì¶œë ¥
uploaded_file = st.file_uploader("ë„¤ì´ë²„ ì‡¼í•‘ ë¶„ì„ ê²°ê³¼ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["csv"])

if uploaded_file:
    df = None
    try:
        df = pd.read_csv(uploaded_file, encoding='cp949')
    except:
        uploaded_file.seek(0)
        try:
            df = pd.read_csv(uploaded_file, encoding='utf-8-sig')
        except Exception as e:
            st.error(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")

    if df is not None:
        manager = SEOManager(df)
        names, specs, tags = manager.run_analysis()

        st.success("âœ¨ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. í‚¤ì›Œë“œ í™•ì¥ì„±ì´ ê·¹ëŒ€í™”ëœ íƒœê·¸ê°€ ì„ ë³„ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # --- ì„¹ì…˜ 1: ìƒí’ˆëª… ---
        st.header("ğŸ·ï¸ 1. ìƒí’ˆëª… í‚¤ì›Œë“œ (NLU ìµœì í™”)")
        col1, col2 = st.columns([2, 1])
        with col1:
            st.subheader("âœ… ì¶”ì²œ ìƒí’ˆëª… ì¡°í•©")
            recommended_title = " ".join([n[0] for n in names])
            st.code(recommended_title, language=None)
            st.markdown(f"**NLU ë¶„ì„ ì „ëµ:** ê°€ì¥ ìœ ì…ëŸ‰ì´ ë§ì€ {len(names)}ê°œì˜ ë…ë¦½ í‚¤ì›Œë“œë¥¼ ë„ì–´ì“°ê¸°ë¡œ ë°°ì¹˜í–ˆìŠµë‹ˆë‹¤.")
        with col2:
            st.subheader("ğŸ“Š ë‹¨ì–´ë³„ ë¹ˆë„")
            name_df = pd.DataFrame(names, columns=['ë‹¨ì–´', 'ë¹ˆë„'])
            st.table(name_df)

        st.markdown("---")

        # --- ì„¹ì…˜ 2: ì†ì„± í‚¤ì›Œë“œ ---
        st.header("âš™ï¸ 2. ê¶Œì¥ ì†ì„± í‚¤ì›Œë“œ (í•„í„° ê²€ìƒ‰ìš©)")
        col3, col4 = st.columns([2, 1])
        with col3:
            st.subheader("âœ… í•„í„° ìµœì í™” ì†ì„±")
            for s, c in specs:
                st.button(f"{s}", key=f"attr_{s}", use_container_width=True)
            st.caption("ì†ì„±ë€ì— ìœ„ ë‹¨ì–´ë“¤ì„ ì²´í¬í•˜ì—¬ í•„í„° ê²€ìƒ‰ ìœ ì…ì„ í™•ë³´í•˜ì„¸ìš”.")
        with col4:
            st.subheader("ğŸ“Š ì†ì„± ì¸ì‹ ë°ì´í„°")
            spec_df = pd.DataFrame(specs, columns=['ì†ì„±ê°’', 'ë¹ˆë„'])
            st.table(spec_df)

        st.markdown("---")

        # --- ì„¹ì…˜ 3: ê²€ìƒ‰ íƒœê·¸ ---
        st.header("ğŸ” 3. í™•ì¥ ê²€ìƒ‰ íƒœê·¸ (í‚¤ì›Œë“œ í™•ì¥ ê·¹ëŒ€í™”)")
        col5, col6 = st.columns([2, 1])
        with col5:
            st.subheader("âœ… ì¤‘ë³µ ë°°ì œ íƒœê·¸ 10ì„ ")
            tag_display = ", ".join([f"#{t[0]}" for t in tags])
            st.warning(tag_display)
            st.info("**í™•ì¥ ë¡œì§ ì ìš©:** '#ì œê³¼ì œë¹µ'ë³´ë‹¤ ë²”ìœ„ê°€ ë„“ì€ '#ì œê³¼ì œë¹µì¬ë£Œ'ë¥¼ ìš°ì„  ì±„íƒí•˜ì—¬ ê²€ìƒ‰ ë²”ìœ„ë¥¼ í™•ì¥í–ˆìŠµë‹ˆë‹¤.")
        with col6:
            st.subheader("ğŸ“Š íƒœê·¸ ê²€ìƒ‰ ë°ì´í„°")
            tag_df = pd.DataFrame(tags, columns=['íƒœê·¸ëª…', 'ë¹ˆë„'])
            st.table(tag_df)

        with st.expander("ğŸ’¡ íƒœê·¸ ì„ ë³„ ë¡œì§ ìƒì„¸ ì„¤ëª… (ì§ì› êµìœ¡ìš©)"):
            st.write("""
            - **í™•ì¥í˜• ìš°ì„  ì›ì¹™:** 'A'ë¼ëŠ” ë‹¨ì–´ê°€ 'Aì¬ë£Œ'ë¼ëŠ” ë‹¨ì–´ì— í¬í•¨ëœë‹¤ë©´, ê²€ìƒ‰ ì—”ì§„ì€ 'Aì¬ë£Œ'ë§Œìœ¼ë¡œë„ 'A'ì˜ ì˜ë¯¸ë¥¼ ì–´ëŠ ì •ë„ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
            - **íš¨ìœ¨ì„± ê·¹ëŒ€í™”:** ë”°ë¼ì„œ ì§§ì€ ë‹¨ì–´ì¸ 'ì œê³¼ì œë¹µ'ì„ ë²„ë¦¬ê³  ë” ê¸´ 'ì œê³¼ì œë¹µì¬ë£Œ'ë¥¼ ì„ íƒí•¨ìœ¼ë¡œì¨, ë‚¨ëŠ” í•œ ìë¦¬ì— ë‹¤ë¥¸ ìœ ìš©í•œ íƒœê·¸(ì˜ˆ: #ì‹ìì¬)ë¥¼ í•˜ë‚˜ ë” ë„£ì„ ìˆ˜ ìˆê²Œ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤.
            - **ê²°ê³¼:** ì´ ë¡œì§ì„ í†µí•´ 10ê°œì˜ íƒœê·¸ë§Œìœ¼ë¡œë„ ì•½ 15~20ê°œ ì´ìƒì˜ í‚¤ì›Œë“œ íš¨ê³¼ë¥¼ ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            """)

else:
    st.info("CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ë¶„ì„ì´ ì‹œì‘ë©ë‹ˆë‹¤.")
