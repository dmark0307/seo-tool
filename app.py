import streamlit as st
import pandas as pd
import re
from collections import Counter

# 1. í˜ì´ì§€ ì„¤ì • ë° ë””ìì¸
st.set_page_config(page_title="ë„¤ì´ë²„ SEO NLU ë§ˆìŠ¤í„°", layout="wide")

st.markdown("""
    <style>
    [data-testid="stSidebar"] { min-width: 320px; max-width: 320px; }
    .block-container { padding-top: 2rem; }
    </style>
    """, unsafe_allow_html=True)

st.title("ğŸš€ ë„¤ì´ë²„ ì‡¼í•‘ SEO í†µí•© ìµœì í™” ë§¤ë‹ˆì €")
st.markdown("---")

class SEOManager:
    def __init__(self, df, user_exclude_list):
        self.df = df
        self.exclude_brands = [
            'ë§¤ì¼', 'ì„œìš¸ìš°ìœ ', 'ì„œìš¸', 'ì—°ì„¸', 'ë‚¨ì–‘', 'ê±´êµ­', 'íŒŒìŠ¤í‡´ë¥´', 'ì¼ë™', 'í›„ë””ìŠ¤', 
            'ì†Œì™€ë‚˜ë¬´', 'ë¹™ê·¸ë ˆ', 'ì…€ë¡œëª¬', 'ë¹…ì›ë”', 'ë¯¸ê´‘ìŠ¤í† ì–´', 'ë°ì–´ë¦¬ë§ˆì¼“', 'ë„ë‚¨ìƒíšŒ', 
            'í¬ì°½ìœ ì—…', 'ë‹´í„°', 'ì—°ì„¸ìœ ì—…', 'ë§¤ì¼ìœ ì—…'
        ] + user_exclude_list
        # ë³µí•© ëª…ì‚¬ ë¶„ë¦¬ìš© ì„œë¸Œ í‚¤ì›Œë“œ
        self.sub_splits = sorted(['ìíŒê¸°', 'ìš°ìœ ', 'ë¶„ìœ ', 'ê°€ë£¨', 'ë¶„ë§', 'ì „ì§€', 'íƒˆì§€', 'ìŠ¤í‹±', 'ì—…ì†Œìš©', 'ëŒ€ìš©ëŸ‰', 'ë©¸ê· ', 'íŒŒìš°ì¹˜', 'ì¶”ì–µ', 'ê°„ì‹', 'ì¬ë£Œ'], key=len, reverse=True)

    def split_base_terms(self, text, is_manual=False):
        """
        NLU ê·œì¹™ì— ë”°ë¼ ë³µí•© ëª…ì‚¬ë¥¼ ë¶„ë¦¬ 
        (is_manual=Trueì¼ ê²½ìš° ìˆ«ì/1ê¸€ìë„ ë³´ì¡´í•˜ì—¬ ì‚¬ìš©ì ì˜ë„ ë°˜ì˜)
        """
        if pd.isna(text) or text == '-': return []
        text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', ' ', str(text))
        raw_words = text.split()
        terms = []
        pattern = f"({'|'.join(self.sub_splits)})"
        
        for word in raw_words:
            if word in self.exclude_brands: continue
            
            # ìˆ˜ë™ ì…ë ¥(is_manual)ì¸ ê²½ìš° ìˆ«ìê°€ í¬í•¨ë˜ì–´ë„ í—ˆìš© (ì˜ˆ: 1kg, 2ê°œ)
            # ìë™ ì¶”ì¶œì¸ ê²½ìš° ìˆ«ìê°€ ìˆìœ¼ë©´ ì œì™¸
            if not is_manual and any(char.isdigit() for char in word): continue
            
            parts = re.split(pattern, word)
            for p in parts:
                p = p.strip()
                if not p or p in self.exclude_brands: continue
                
                # ìˆ˜ë™ ì…ë ¥ì€ 1ê¸€ìë„ í—ˆìš©, ìë™ ì¶”ì¶œì€ 2ê¸€ì ì´ìƒë§Œ í—ˆìš©
                if is_manual or len(p) > 1 or p in self.sub_splits:
                    terms.append(p)
        return terms

    def extract_stats_data(self, stats_df, target_product_code):
        """í†µê³„ íŒŒì¼ì—ì„œ íŠ¹ì • ìƒí’ˆ ì½”ë“œì˜ ìœ ì… í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            # ì»¬ëŸ¼ëª… ìœ ì—°í•˜ê²Œ ì°¾ê¸°
            code_col = [c for c in stats_df.columns if any(x in c for x in ['ë²ˆí˜¸', 'ID', 'ì½”ë“œ'])][0]
            kw_col = [c for c in stats_df.columns if 'í‚¤ì›Œë“œ' in c][0]
            name_col = [c for c in stats_df.columns if 'ìƒí’ˆëª…' in c][0]
            
            filtered_df = stats_df[stats_df[code_col].astype(str) == str(target_product_code)]
            if filtered_df.empty: return [], ""
            
            existing_name = str(filtered_df[name_col].iloc[0])
            raw_keywords = filtered_df[kw_col].dropna().unique().tolist()
            
            extracted = []
            for rk in raw_keywords:
                if rk != '-': extracted.extend(self.split_base_terms(rk))
            
            # ìƒìœ„ 5ê°œë§Œ ì¶”ì¶œ (ì¤‘ë³µ ì œê±°)
            return list(dict.fromkeys(extracted))[:5], existing_name
        except:
            return [], ""

    def reorder_for_readability(self, word_count_pairs):
        """ìë™ ì¶”ì²œ í‚¤ì›Œë“œ ê°€ë…ì„± ì •ë ¬"""
        identity, form, usage, desc = ['ì „ì§€', 'ë¶„ìœ ', 'ìš°ìœ ', 'íƒˆì§€'], ['ë¶„ë§', 'ê°€ë£¨', 'ìŠ¤í‹±', 'ì•¡ìƒ'], ['ìíŒê¸°', 'ì—…ì†Œìš©', 'ëŒ€ìš©ëŸ‰', 'ì‹ìì¬'], ['ì§„í•œ', 'ê³ ì†Œí•œ', 'ë§›ìˆëŠ”', 'ì¶”ì–µ']
        def get_priority(pair):
            word = pair[0]
            if any(core in word for core in identity): return 1
            if any(core in word for core in form): return 2
            if any(core in word for core in usage): return 3
            if any(core in word for core in desc): return 4
            return 5
        return sorted(word_count_pairs, key=lambda x: get_priority(x))

    def run_analysis(self, stats_keywords, fixed_input, conversion_input, total_target_count):
        # 1. ì‚¬ìš©ì ì…ë ¥ í‚¤ì›Œë“œ ì²˜ë¦¬ (is_manual=Trueë¡œ ë³´í˜¸)
        # fixed_input: "ê³ ì • ë°°ì¹˜"
        # conversion_input: "êµ¬ë§¤ì „í™˜ ì¶”ê°€"
        manual_fixed = self.split_base_terms(fixed_input, is_manual=True)
        manual_conv = self.split_base_terms(conversion_input, is_manual=True)
        
        # 2. [ì¤‘ìš”] ìˆœì„œ ê²°ì •: í†µê³„ -> ê³ ì • ë°°ì¹˜ -> êµ¬ë§¤ì „í™˜ ì¶”ê°€
        combined_keywords = []
        
        # (1) í†µê³„ í‚¤ì›Œë“œ
        for k in stats_keywords:
            if k not in combined_keywords: combined_keywords.append(k)
            
        # (2) ê³ ì • ë°°ì¹˜ í‚¤ì›Œë“œ
        for k in manual_fixed:
            if k not in combined_keywords: combined_keywords.append(k)
            
        # (3) êµ¬ë§¤ì „í™˜ ì¶”ê°€ í‚¤ì›Œë“œ
        for k in manual_conv:
            if k not in combined_keywords: combined_keywords.append(k)
            
        fixed_result = combined_keywords[:] # ìµœì¢… ê³ ì • ë¦¬ìŠ¤íŠ¸

        # 3. ìƒí’ˆëª… ìë™ ì¶”ì¶œ (ë‚¨ì€ ìë¦¬ ì±„ìš°ê¸°)
        name_terms = []
        for name in self.df['ìƒí’ˆëª…']: name_terms.extend(self.split_base_terms(name))
        name_freq = Counter(name_terms).most_common(100)
        
        auto_candidates = [w for w, c in name_freq if w not in fixed_result]
        remain_count = max(0, total_target_count - len(fixed_result))
        
        selected_auto = auto_candidates[:remain_count]
        readable_auto_pairs = self.reorder_for_readability([(w, Counter(name_terms)[w]) for w in selected_auto])
        
        # 4. ì†ì„± ë¶„ì„
        spec_list = []
        for spec in self.df['ìŠ¤í™'].dropna():
            parts = [p.strip() for p in str(spec).split('|')]
            spec_list.extend([p for p in parts if len(p) > 1 and p not in self.exclude_brands])
        spec_counts = Counter(spec_list).most_common(8)

        # 5. íƒœê·¸ ë¶„ì„ (100% ì¼ì¹˜ ì¤‘ë³µë§Œ ì œê±°)
        tag_raw_list = []
        for tags in self.df['ê²€ìƒ‰ì¸ì‹íƒœê·¸'].dropna():
            if tags != '-': tag_raw_list.extend([t.strip() for t in str(tags).split(',') if t.strip()])
        tag_freq = Counter(tag_raw_list).most_common(300)
        
        title_keywords = set(fixed_result + [p[0] for p in readable_auto_pairs])
        
        final_tags = []
        for i, (t_raw, c) in enumerate(tag_freq):
            if len(final_tags) >= 10: break
            
            # ê¸°ë³¸ í•„í„°ë§: ë¸Œëœë“œ ì œì™¸, ìˆ«ì ì œì™¸, ìƒí’ˆëª…ì— ìˆëŠ” ë‹¨ì–´ ì œì™¸
            if any(brand in t_raw for brand in self.exclude_brands) or any(char.isdigit() for char in t_raw): continue
            if t_raw in title_keywords: continue 

            # ì¤‘ë³µ ì œê±° ë¡œì§ (100% ì¼ì¹˜í•  ë•Œë§Œ ì œê±°)
            is_exact_dup = False
            for existing_t, _ in final_tags:
                if t_raw == existing_t:
                    is_exact_dup = True
                    break
            
            if not is_exact_dup:
                final_tags.append((t_raw, c))

        return fixed_result, readable_auto_pairs, spec_counts, final_tags

def calculate_seo_metrics(text):
    """ê¸€ììˆ˜ ë° ë°”ì´íŠ¸ ìˆ˜ ê³„ì‚° (ë„¤ì´ë²„ ê¸°ì¤€)"""
    c_len = len(text)
    try: b_len = len(text.encode('euc-kr'))
    except: b_len = len(text.encode('utf-8'))
    return c_len, b_len

# --- ì‚¬ì´ë“œë°” UI ---
with st.sidebar:
    st.subheader("âš™ï¸ ë¶„ì„ ì„¤ì •")
    with st.expander("ğŸ“ 1. ë°ì´í„° ì†ŒìŠ¤", expanded=True):
        uploaded_file = st.file_uploader("ìƒí’ˆ(CSV)", type=["csv"])
        stats_file = st.file_uploader("í†µê³„(XL/CSV)", type=["csv", "xlsx"])
        target_code = st.text_input("ğŸ¯ ì½”ë“œ", placeholder="ìƒí’ˆì½”ë“œ ì…ë ¥")

    with st.expander("ğŸ¯ 2. ì „ëµ ì„¤ì • (ìˆœì„œ ì¤‘ìš”)", expanded=True):
        st.info("ì¡°í•© ìˆœì„œ: [í†µê³„] â†’ [ê³ ì •] â†’ [êµ¬ë§¤ì „í™˜]")
        add_input = st.text_input("ğŸ“Œ ê³ ì • ë°°ì¹˜", placeholder="ì˜ˆ: ë¬´ë£Œë°°ì†¡")
        conversion_input = st.text_input("â• êµ¬ë§¤ì „í™˜ ì¶”ê°€", placeholder="ì˜ˆ: ë§›ìˆëŠ”")
        total_kw_count = st.number_input("ğŸ”¢ ëª©í‘œ í‚¤ì›Œë“œ ìˆ˜", min_value=5, value=11)
        exclude_input = st.text_input("ğŸš« ì œì™¸ í‚¤ì›Œë“œ", placeholder="ë¸Œëœë“œëª… ë“±")

    user_exclude = [w.strip() for w in exclude_input.split() if w.strip()]

if uploaded_file:
    # íŒŒì¼ í¬ì¸í„° ì´ˆê¸°í™” ë° ë¡œë“œ
    try:
        uploaded_file.seek(0)
        df = pd.read_csv(uploaded_file, encoding='cp949')
    except:
        uploaded_file.seek(0)
        df = pd.read_csv(uploaded_file, encoding='utf-8-sig')

    manager = SEOManager(df, user_exclude)
    stats_kws, old_name = [], ""
    
    # í†µê³„ íŒŒì¼ ì²˜ë¦¬
    if stats_file and target_code:
        try:
            stats_file.seek(0)
            if stats_file.name.endswith('.csv'):
                stats_df = pd.read_csv(stats_file, encoding='cp949')
            else:
                stats_df = pd.read_excel(stats_file, engine='openpyxl')
            
            stats_kws, old_name = manager.extract_stats_data(stats_df, target_code)
            if stats_kws: st.sidebar.success(f"âœ”ï¸ í†µê³„ í‚¤ì›Œë“œ {len(stats_kws)}ê°œ í™•ë³´")
        except Exception as e: 
            st.sidebar.error(f"í†µê³„ ë¶„ì„ ì˜¤ë¥˜: {e}")

    # ë¶„ì„ ì‹¤í–‰ (ìˆœì„œ: í†µê³„ -> ê³ ì •(add) -> ì „í™˜(conv))
    fixed, auto, specs, tags = manager.run_analysis(stats_kws, add_input, conversion_input, total_kw_count)

    # 1. ì „ëµì  ìƒí’ˆëª… ì¡°í•©
    st.header("ğŸ·ï¸ 1. ì „ëµì  ìƒí’ˆëª… ì¡°í•©")
    col1, col2 = st.columns([2, 1])
    with col1:
        if old_name: st.info(f"ğŸ“ **ê¸°ì¡´ ìƒí’ˆëª…:** {old_name}")
        st.subheader("âœ… ì™„ì„±ëœ ìƒí’ˆëª…")
        
        full_title = " ".join(fixed + [p[0] for p in auto])
        c_len, b_len = calculate_seo_metrics(full_title)
        
        # 50ì ê²€ì¦ ë¡œì§
        if c_len <= 50:
            st.code(full_title, language=None)
            st.markdown(f"ğŸŸ¢ **ì •ìƒ (50ì ì´ë‚´)**: {c_len}ì / {b_len} Byte")
        else:
            st.code(full_title, language=None)
            st.markdown(f"ğŸ”´ **ì£¼ì˜ (50ì ì´ˆê³¼)**: {c_len}ì ({c_len-50}ì ì´ˆê³¼) / {b_len} Byte")
            st.warning("âš ï¸ ìƒí’ˆëª…ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤. ì…ë ¥í•œ í‚¤ì›Œë“œë¥¼ ì¤„ì´ê±°ë‚˜ ëª©í‘œ í‚¤ì›Œë“œ ìˆ˜ë¥¼ ì¡°ì ˆí•˜ì„¸ìš”.")

        st.markdown(f"**ìˆœì„œ ê²€ì¦:** `{' + '.join(fixed)}` (í†µê³„/ê³ ì •/ì „í™˜) + `{' '.join([p[0] for p in auto])}` (ìë™)")

    with col2:
        st.subheader("ğŸ“Š ìë™ ì¶”ì²œ ë¹ˆë„")
        st.table(pd.DataFrame(auto, columns=['ë‹¨ì–´', 'ë¹ˆë„']).assign(No=range(1, len(auto)+1)).set_index('No'))

    st.markdown("---")
    # 2. í•„í„° ë…¸ì¶œìš© ì†ì„±ê°’
    st.header("âš™ï¸ 2. í•„í„° ë…¸ì¶œìš© ì†ì„±ê°’")
    col3, col4 = st.columns([2, 1])
    with col3:
        for s, _ in specs: st.button(s, key=f"attr_{s}", use_container_width=True)
    with col4:
        st.table(pd.DataFrame(specs, columns=['ì†ì„±ê°’', 'ë¹ˆë„']).set_index(pd.Index(range(1, len(specs)+1))))

    st.markdown("---")
    # 3. í™•ì¥ ê²€ìƒ‰ íƒœê·¸
    st.header("ğŸ” 3. í™•ì¥ ê²€ìƒ‰ íƒœê·¸ (ì¤‘ë³µ ìµœì†Œí™”)")
    col5, col6 = st.columns([2, 1])
    with col5:
        st.subheader("âœ… ìµœì í™” íƒœê·¸ 10ì„ ")
        st.success(", ".join([f"#{t[0]}" for t in tags]))
        st.caption("â€» 100% ì¼ì¹˜í•˜ëŠ” íƒœê·¸ë§Œ ì¤‘ë³µìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ë‹¤ì–‘í•œ í‚¤ì›Œë“œ ì¡°í•©ì„ ìœ ì§€í–ˆìŠµë‹ˆë‹¤.")
    with col6:
        st.subheader("ğŸ“Š íƒœê·¸ ì‚¬ìš© ë¹ˆë„ìˆ˜")
        st.table(pd.DataFrame(tags, columns=['íƒœê·¸ëª…', 'ì‚¬ìš© ë¹ˆë„ìˆ˜']).assign(No=range(1, len(tags)+1)).set_index('No'))
else:
    st.info("ì¢Œì¸¡ ë©”ë‰´ì—ì„œ ìƒí’ˆ ë°ì´í„° íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
